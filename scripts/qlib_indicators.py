#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import pandas as pd
import numpy as np
import talib
import struct
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from loguru import logger
import warnings
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
from functools import partial
import multiprocessing

warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=FutureWarning)


class QlibIndicatorsEnhancedCalculator:
    """
    å¢å¼ºç‰ˆQlibæŒ‡æ ‡è®¡ç®—å™¨
    é›†æˆAlpha158ã€Alpha360æŒ‡æ ‡ä½“ç³»å’Œå¤šç§æŠ€æœ¯åˆ†ææŒ‡æ ‡
    æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—å’ŒæŒ‡æ ‡å»é‡åŠŸèƒ½
    """
    
    # å­—æ®µä¸­æ–‡æ ‡ç­¾æ˜ å°„
    FIELD_LABELS = {
        # åŸºç¡€å­—æ®µ
        'Date': 'æ—¥æœŸ',
        'Symbol': 'è‚¡ç¥¨ä»£ç ',
        
        # OHLCVåŸºç¡€æ•°æ®
        'Open': 'å¼€ç›˜ä»·',
        'High': 'æœ€é«˜ä»·', 
        'Low': 'æœ€ä½ä»·',
        'Close': 'æ”¶ç›˜ä»·',
        'Volume': 'æˆäº¤é‡',
        
        # Alpha158æŒ‡æ ‡ä½“ç³»
        'ALPHA158_KMID': 'Kçº¿ä¸­ç‚¹ä»·æ ¼',
        'ALPHA158_KLEN': 'Kçº¿é•¿åº¦',
        'ALPHA158_KMID2': 'Kçº¿ä¸­ç‚¹ä»·æ ¼å¹³æ–¹',
        'ALPHA158_KUP': 'Kçº¿ä¸Šå½±çº¿',
        'ALPHA158_KUP2': 'Kçº¿ä¸Šå½±çº¿å¹³æ–¹',
        'ALPHA158_KLOW': 'Kçº¿ä¸‹å½±çº¿',
        'ALPHA158_KLOW2': 'Kçº¿ä¸‹å½±çº¿å¹³æ–¹',
        'ALPHA158_KSFT': 'Kçº¿åç§»',
        'ALPHA158_KSFT2': 'Kçº¿åç§»å¹³æ–¹',
        'ALPHA158_ROC_1': '1æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_2': '2æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_3': '3æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_4': '4æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_5': '5æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_10': '10æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_20': '20æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_30': '30æ—¥æ”¶ç›Šç‡',
        'ALPHA158_ROC_60': '60æ—¥æ”¶ç›Šç‡',
        'ALPHA158_MA_5': '5æ—¥ç§»åŠ¨å¹³å‡',
        'ALPHA158_MA_10': '10æ—¥ç§»åŠ¨å¹³å‡',
        'ALPHA158_MA_20': '20æ—¥ç§»åŠ¨å¹³å‡',
        'ALPHA158_MA_30': '30æ—¥ç§»åŠ¨å¹³å‡',
        'ALPHA158_MA_60': '60æ—¥ç§»åŠ¨å¹³å‡',
        'ALPHA158_STD_5': '5æ—¥æ ‡å‡†å·®',
        'ALPHA158_STD_10': '10æ—¥æ ‡å‡†å·®',
        'ALPHA158_STD_20': '20æ—¥æ ‡å‡†å·®',
        'ALPHA158_STD_30': '30æ—¥æ ‡å‡†å·®',
        'ALPHA158_STD_60': '60æ—¥æ ‡å‡†å·®',
        'ALPHA158_BETA_5': '5æ—¥è´å¡”å€¼',
        'ALPHA158_BETA_10': '10æ—¥è´å¡”å€¼',
        'ALPHA158_BETA_20': '20æ—¥è´å¡”å€¼',
        'ALPHA158_BETA_30': '30æ—¥è´å¡”å€¼',
        'ALPHA158_BETA_60': '60æ—¥è´å¡”å€¼',
        'ALPHA158_RSQR_5': '5æ—¥Rå¹³æ–¹',
        'ALPHA158_RSQR_10': '10æ—¥Rå¹³æ–¹',
        'ALPHA158_RSQR_20': '20æ—¥Rå¹³æ–¹',
        'ALPHA158_RSQR_30': '30æ—¥Rå¹³æ–¹',
        'ALPHA158_RSQR_60': '60æ—¥Rå¹³æ–¹',
        'ALPHA158_RESI_5': '5æ—¥æ®‹å·®',
        'ALPHA158_RESI_10': '10æ—¥æ®‹å·®',
        'ALPHA158_RESI_20': '20æ—¥æ®‹å·®',
        'ALPHA158_RESI_30': '30æ—¥æ®‹å·®',
        'ALPHA158_RESI_60': '60æ—¥æ®‹å·®',
        'ALPHA158_MAX_5': '5æ—¥æœ€å¤§å€¼',
        'ALPHA158_MAX_10': '10æ—¥æœ€å¤§å€¼',
        'ALPHA158_MAX_20': '20æ—¥æœ€å¤§å€¼',
        'ALPHA158_MAX_30': '30æ—¥æœ€å¤§å€¼',
        'ALPHA158_MAX_60': '60æ—¥æœ€å¤§å€¼',
        'ALPHA158_MIN_5': '5æ—¥æœ€å°å€¼',
        'ALPHA158_MIN_10': '10æ—¥æœ€å°å€¼',
        'ALPHA158_MIN_20': '20æ—¥æœ€å°å€¼',
        'ALPHA158_MIN_30': '30æ—¥æœ€å°å€¼',
        'ALPHA158_MIN_60': '60æ—¥æœ€å°å€¼',
        'ALPHA158_QTLU_5': '5æ—¥ä¸Šå››åˆ†ä½æ•°',
        'ALPHA158_QTLU_10': '10æ—¥ä¸Šå››åˆ†ä½æ•°',
        'ALPHA158_QTLU_20': '20æ—¥ä¸Šå››åˆ†ä½æ•°',
        'ALPHA158_QTLU_30': '30æ—¥ä¸Šå››åˆ†ä½æ•°',
        'ALPHA158_QTLU_60': '60æ—¥ä¸Šå››åˆ†ä½æ•°',
        'ALPHA158_QTLD_5': '5æ—¥ä¸‹å››åˆ†ä½æ•°',
        'ALPHA158_QTLD_10': '10æ—¥ä¸‹å››åˆ†ä½æ•°',
        'ALPHA158_QTLD_20': '20æ—¥ä¸‹å››åˆ†ä½æ•°',
        'ALPHA158_QTLD_30': '30æ—¥ä¸‹å››åˆ†ä½æ•°',
        'ALPHA158_QTLD_60': '60æ—¥ä¸‹å››åˆ†ä½æ•°',
        'ALPHA158_RANK_5': '5æ—¥æ’å',
        'ALPHA158_RANK_10': '10æ—¥æ’å',
        'ALPHA158_RANK_20': '20æ—¥æ’å',
        'ALPHA158_RANK_30': '30æ—¥æ’å',
        'ALPHA158_RANK_60': '60æ—¥æ’å',
        'ALPHA158_RSV_5': '5æ—¥RSV',
        'ALPHA158_RSV_10': '10æ—¥RSV',
        'ALPHA158_RSV_20': '20æ—¥RSV',
        'ALPHA158_RSV_30': '30æ—¥RSV',
        'ALPHA158_RSV_60': '60æ—¥RSV',
        'ALPHA158_IMAX_5': '5æ—¥æœ€å¤§å€¼ä½ç½®',
        'ALPHA158_IMAX_10': '10æ—¥æœ€å¤§å€¼ä½ç½®',
        'ALPHA158_IMAX_20': '20æ—¥æœ€å¤§å€¼ä½ç½®',
        'ALPHA158_IMAX_30': '30æ—¥æœ€å¤§å€¼ä½ç½®',
        'ALPHA158_IMAX_60': '60æ—¥æœ€å¤§å€¼ä½ç½®',
        'ALPHA158_IMIN_5': '5æ—¥æœ€å°å€¼ä½ç½®',
        'ALPHA158_IMIN_10': '10æ—¥æœ€å°å€¼ä½ç½®',
        'ALPHA158_IMIN_20': '20æ—¥æœ€å°å€¼ä½ç½®',
        'ALPHA158_IMIN_30': '30æ—¥æœ€å°å€¼ä½ç½®',
        'ALPHA158_IMIN_60': '60æ—¥æœ€å°å€¼ä½ç½®',
        'ALPHA158_IMXD_5': '5æ—¥æœ€å¤§æœ€å°å€¼å·®ä½ç½®',
        'ALPHA158_IMXD_10': '10æ—¥æœ€å¤§æœ€å°å€¼å·®ä½ç½®',
        'ALPHA158_IMXD_20': '20æ—¥æœ€å¤§æœ€å°å€¼å·®ä½ç½®',
        'ALPHA158_IMXD_30': '30æ—¥æœ€å¤§æœ€å°å€¼å·®ä½ç½®',
        'ALPHA158_IMXD_60': '60æ—¥æœ€å¤§æœ€å°å€¼å·®ä½ç½®',
        'ALPHA158_CORR_5': '5æ—¥ç›¸å…³æ€§',
        'ALPHA158_CORR_10': '10æ—¥ç›¸å…³æ€§',
        'ALPHA158_CORR_20': '20æ—¥ç›¸å…³æ€§',
        'ALPHA158_CORR_30': '30æ—¥ç›¸å…³æ€§',
        'ALPHA158_CORR_60': '60æ—¥ç›¸å…³æ€§',
        'ALPHA158_CORD_5': '5æ—¥ç›¸å…³æ€§å·®åˆ†',
        'ALPHA158_CORD_10': '10æ—¥ç›¸å…³æ€§å·®åˆ†',
        'ALPHA158_CORD_20': '20æ—¥ç›¸å…³æ€§å·®åˆ†',
        'ALPHA158_CORD_30': '30æ—¥ç›¸å…³æ€§å·®åˆ†',
        'ALPHA158_CORD_60': '60æ—¥ç›¸å…³æ€§å·®åˆ†',
        
        # Alpha158æ–°å¢æŒ‡æ ‡
        'ALPHA158_RESI_5': '5æ—¥çº¿æ€§å›å½’æ®‹å·®',
        'ALPHA158_RESI_10': '10æ—¥çº¿æ€§å›å½’æ®‹å·®',
        'ALPHA158_RESI_20': '20æ—¥çº¿æ€§å›å½’æ®‹å·®',
        'ALPHA158_RESI_30': '30æ—¥çº¿æ€§å›å½’æ®‹å·®',
        'ALPHA158_RESI_60': '60æ—¥çº¿æ€§å›å½’æ®‹å·®',
        'ALPHA158_IMAX_5': '5æ—¥æœ€é«˜ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMAX_10': '10æ—¥æœ€é«˜ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMAX_20': '20æ—¥æœ€é«˜ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMAX_30': '30æ—¥æœ€é«˜ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMAX_60': '60æ—¥æœ€é«˜ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMIN_5': '5æ—¥æœ€ä½ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMIN_10': '10æ—¥æœ€ä½ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMIN_20': '20æ—¥æœ€ä½ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMIN_30': '30æ—¥æœ€ä½ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMIN_60': '60æ—¥æœ€ä½ä»·ä½ç½®æŒ‡æ•°',
        'ALPHA158_IMXD_5': '5æ—¥æœ€é«˜æœ€ä½ä»·ä½ç½®å·®',
        'ALPHA158_IMXD_10': '10æ—¥æœ€é«˜æœ€ä½ä»·ä½ç½®å·®',
        'ALPHA158_IMXD_20': '20æ—¥æœ€é«˜æœ€ä½ä»·ä½ç½®å·®',
        'ALPHA158_IMXD_30': '30æ—¥æœ€é«˜æœ€ä½ä»·ä½ç½®å·®',
        'ALPHA158_IMXD_60': '60æ—¥æœ€é«˜æœ€ä½ä»·ä½ç½®å·®',
        'ALPHA158_CNTP_5': '5æ—¥ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTP_10': '10æ—¥ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTP_20': '20æ—¥ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTP_30': '30æ—¥ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTP_60': '60æ—¥ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTN_5': '5æ—¥ä¸‹è·Œå¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTN_10': '10æ—¥ä¸‹è·Œå¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTN_20': '20æ—¥ä¸‹è·Œå¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTN_30': '30æ—¥ä¸‹è·Œå¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTN_60': '60æ—¥ä¸‹è·Œå¤©æ•°æ¯”ä¾‹',
        'ALPHA158_CNTD_5': '5æ—¥æ¶¨è·Œå¤©æ•°å·®',
        'ALPHA158_CNTD_10': '10æ—¥æ¶¨è·Œå¤©æ•°å·®',
        'ALPHA158_CNTD_20': '20æ—¥æ¶¨è·Œå¤©æ•°å·®',
        'ALPHA158_CNTD_30': '30æ—¥æ¶¨è·Œå¤©æ•°å·®',
        'ALPHA158_CNTD_60': '60æ—¥æ¶¨è·Œå¤©æ•°å·®',
        'ALPHA158_SUMP_5': '5æ—¥æ€»æ”¶ç›Šæ¯”ä¾‹',
        'ALPHA158_SUMP_10': '10æ—¥æ€»æ”¶ç›Šæ¯”ä¾‹',
        'ALPHA158_SUMP_20': '20æ—¥æ€»æ”¶ç›Šæ¯”ä¾‹',
        'ALPHA158_SUMP_30': '30æ—¥æ€»æ”¶ç›Šæ¯”ä¾‹',
        'ALPHA158_SUMP_60': '60æ—¥æ€»æ”¶ç›Šæ¯”ä¾‹',
        'ALPHA158_SUMN_5': '5æ—¥æ€»æŸå¤±æ¯”ä¾‹',
        'ALPHA158_SUMN_10': '10æ—¥æ€»æŸå¤±æ¯”ä¾‹',
        'ALPHA158_SUMN_20': '20æ—¥æ€»æŸå¤±æ¯”ä¾‹',
        'ALPHA158_SUMN_30': '30æ—¥æ€»æŸå¤±æ¯”ä¾‹',
        'ALPHA158_SUMN_60': '60æ—¥æ€»æŸå¤±æ¯”ä¾‹',
        'ALPHA158_SUMD_5': '5æ—¥æ”¶ç›ŠæŸå¤±å·®æ¯”ä¾‹',
        'ALPHA158_SUMD_10': '10æ—¥æ”¶ç›ŠæŸå¤±å·®æ¯”ä¾‹',
        'ALPHA158_SUMD_20': '20æ—¥æ”¶ç›ŠæŸå¤±å·®æ¯”ä¾‹',
        'ALPHA158_SUMD_30': '30æ—¥æ”¶ç›ŠæŸå¤±å·®æ¯”ä¾‹',
        'ALPHA158_SUMD_60': '60æ—¥æ”¶ç›ŠæŸå¤±å·®æ¯”ä¾‹',
        'ALPHA158_VMA_5': '5æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡',
        'ALPHA158_VMA_10': '10æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡',
        'ALPHA158_VMA_20': '20æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡',
        'ALPHA158_VMA_30': '30æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡',
        'ALPHA158_VMA_60': '60æ—¥æˆäº¤é‡ç§»åŠ¨å¹³å‡',
        'ALPHA158_VSTD_5': '5æ—¥æˆäº¤é‡æ ‡å‡†å·®',
        'ALPHA158_VSTD_10': '10æ—¥æˆäº¤é‡æ ‡å‡†å·®',
        'ALPHA158_VSTD_20': '20æ—¥æˆäº¤é‡æ ‡å‡†å·®',
        'ALPHA158_VSTD_30': '30æ—¥æˆäº¤é‡æ ‡å‡†å·®',
        'ALPHA158_VSTD_60': '60æ—¥æˆäº¤é‡æ ‡å‡†å·®',
        'ALPHA158_WVMA_5': '5æ—¥æˆäº¤é‡åŠ æƒä»·æ ¼æ³¢åŠ¨ç‡',
        'ALPHA158_WVMA_10': '10æ—¥æˆäº¤é‡åŠ æƒä»·æ ¼æ³¢åŠ¨ç‡',
        'ALPHA158_WVMA_20': '20æ—¥æˆäº¤é‡åŠ æƒä»·æ ¼æ³¢åŠ¨ç‡',
        'ALPHA158_WVMA_30': '30æ—¥æˆäº¤é‡åŠ æƒä»·æ ¼æ³¢åŠ¨ç‡',
        'ALPHA158_WVMA_60': '60æ—¥æˆäº¤é‡åŠ æƒä»·æ ¼æ³¢åŠ¨ç‡',
        'ALPHA158_VSUMP_5': '5æ—¥æˆäº¤é‡ä¸Šå‡æ¯”ä¾‹',
        'ALPHA158_VSUMP_10': '10æ—¥æˆäº¤é‡ä¸Šå‡æ¯”ä¾‹',
        'ALPHA158_VSUMP_20': '20æ—¥æˆäº¤é‡ä¸Šå‡æ¯”ä¾‹',
        'ALPHA158_VSUMP_30': '30æ—¥æˆäº¤é‡ä¸Šå‡æ¯”ä¾‹',
        'ALPHA158_VSUMP_60': '60æ—¥æˆäº¤é‡ä¸Šå‡æ¯”ä¾‹',
        'ALPHA158_VSUMN_5': '5æ—¥æˆäº¤é‡ä¸‹é™æ¯”ä¾‹',
        'ALPHA158_VSUMN_10': '10æ—¥æˆäº¤é‡ä¸‹é™æ¯”ä¾‹',
        'ALPHA158_VSUMN_20': '20æ—¥æˆäº¤é‡ä¸‹é™æ¯”ä¾‹',
        'ALPHA158_VSUMN_30': '30æ—¥æˆäº¤é‡ä¸‹é™æ¯”ä¾‹',
        'ALPHA158_VSUMN_60': '60æ—¥æˆäº¤é‡ä¸‹é™æ¯”ä¾‹',
        'ALPHA158_VSUMD_5': '5æ—¥æˆäº¤é‡æ¶¨è·Œå·®æ¯”ä¾‹',
        'ALPHA158_VSUMD_10': '10æ—¥æˆäº¤é‡æ¶¨è·Œå·®æ¯”ä¾‹',
        'ALPHA158_VSUMD_20': '20æ—¥æˆäº¤é‡æ¶¨è·Œå·®æ¯”ä¾‹',
        'ALPHA158_VSUMD_30': '30æ—¥æˆäº¤é‡æ¶¨è·Œå·®æ¯”ä¾‹',
        'ALPHA158_VSUMD_60': '60æ—¥æˆäº¤é‡æ¶¨è·Œå·®æ¯”ä¾‹',
        
        # æŠ€æœ¯æŒ‡æ ‡
        'SMA_5': '5æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'SMA_10': '10æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'SMA_20': '20æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'SMA_50': '50æ—¥ç®€å•ç§»åŠ¨å¹³å‡',
        'EMA_5': '5æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'EMA_10': '10æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'EMA_20': '20æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'EMA_50': '50æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'DEMA_20': '20æ—¥åŒé‡æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'TEMA_20': '20æ—¥ä¸‰é‡æŒ‡æ•°ç§»åŠ¨å¹³å‡',
        'KAMA_30': '30æ—¥è‡ªé€‚åº”ç§»åŠ¨å¹³å‡',
        'WMA_20': '20æ—¥åŠ æƒç§»åŠ¨å¹³å‡',
        'MACD': 'MACDä¸»çº¿',
        'MACD_Signal': 'MACDä¿¡å·çº¿',
        'MACD_Histogram': 'MACDæŸ±çŠ¶å›¾',
        'MACDEXT': 'MACDæ‰©å±•',
        'MACDFIX': 'MACDå›ºå®š',
        'RSI_14': '14æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ•°',
        'CCI_14': '14æ—¥å•†å“é€šé“æŒ‡æ•°',
        'CMO_14': '14æ—¥é’±å¾·åŠ¨é‡æ‘†åŠ¨æŒ‡æ ‡',
        'MFI_14': '14æ—¥èµ„é‡‘æµé‡æŒ‡æ•°',
        'WILLR_14': '14æ—¥å¨å»‰æŒ‡æ ‡',
        'ULTOSC': 'ç»ˆæéœ‡è¡æŒ‡æ ‡',
        'ADX_14': '14æ—¥å¹³å‡è¶‹å‘æŒ‡æ•°',
        'ADXR_14': '14æ—¥å¹³å‡è¶‹å‘æŒ‡æ•°è¯„çº§',
        'APO': 'ç»å¯¹ä»·æ ¼éœ‡è¡æŒ‡æ ‡',
        'AROON_DOWN': 'é˜¿éš†ä¸‹é™æŒ‡æ ‡',
        'AROON_UP': 'é˜¿éš†ä¸Šå‡æŒ‡æ ‡',
        'AROONOSC_14': '14æ—¥é˜¿éš†éœ‡è¡æŒ‡æ ‡',
        'BOP': 'å‡åŠ¿æŒ‡æ ‡',
        'DX_14': '14æ—¥è¶‹å‘æŒ‡æ•°',
        'MINUS_DI_14': '14æ—¥è´Ÿè¶‹å‘æŒ‡æ ‡',
        'MINUS_DM_14': '14æ—¥è´Ÿå‘è¿åŠ¨',
        'PLUS_DI_14': '14æ—¥æ­£è¶‹å‘æŒ‡æ ‡',
        'PLUS_DM_14': '14æ—¥æ­£å‘è¿åŠ¨',
        'PPO': 'ä»·æ ¼éœ‡è¡ç™¾åˆ†æ¯”',
        'TRIX_30': '30æ—¥ä¸‰é‡æŒ‡æ•°å¹³å‡è¶‹åŠ¿',
        'MOM_10': '10æ—¥åŠ¨é‡æŒ‡æ ‡',
        'ROC_10': '10æ—¥å˜åŠ¨ç‡',
        'ROCP_10': '10æ—¥å˜åŠ¨ç‡ç™¾åˆ†æ¯”',
        'ROCR_10': '10æ—¥å˜åŠ¨ç‡æ¯”ç‡',
        'ROCR100_10': '10æ—¥å˜åŠ¨ç‡æ¯”ç‡100',
        'BB_Upper': 'å¸ƒæ—å¸¦ä¸Šè½¨',
        'BB_Middle': 'å¸ƒæ—å¸¦ä¸­è½¨',
        'BB_Lower': 'å¸ƒæ—å¸¦ä¸‹è½¨',
        'STOCH_K': 'éšæœºæŒ‡æ ‡Kå€¼',
        'STOCH_D': 'éšæœºæŒ‡æ ‡Då€¼',
        'STOCHF_K': 'å¿«é€ŸéšæœºæŒ‡æ ‡Kå€¼',
        'STOCHF_D': 'å¿«é€ŸéšæœºæŒ‡æ ‡Då€¼',
        'STOCHRSI_K': 'éšæœºRSI Kå€¼',
        'STOCHRSI_D': 'éšæœºRSI Då€¼',
        'ATR_14': '14æ—¥å¹³å‡çœŸå®èŒƒå›´',
        'NATR_14': '14æ—¥æ ‡å‡†åŒ–å¹³å‡çœŸå®èŒƒå›´',
        'TRANGE': 'çœŸå®èŒƒå›´',
        'AD': 'ç´¯ç§¯/æ´¾å‘çº¿',
        'ADOSC': 'ç´¯ç§¯/æ´¾å‘éœ‡è¡æŒ‡æ ‡',
        'OBV': 'èƒ½é‡æ½®æŒ‡æ ‡',
        'HT_DCPERIOD': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢ä¸»å¯¼å‘¨æœŸ',
        'HT_DCPHASE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢ä¸»å¯¼ç›¸ä½',
        'HT_PHASOR_INPHASE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢ç›¸ä½å™¨åŒç›¸åˆ†é‡',
        'HT_PHASOR_QUADRATURE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢ç›¸ä½å™¨æ­£äº¤åˆ†é‡',
        'HT_SINE_SINE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢æ­£å¼¦æ³¢',
        'HT_SINE_LEADSINE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢å‰å¯¼æ­£å¼¦æ³¢',
        'HT_TRENDMODE': 'å¸Œå°”ä¼¯ç‰¹å˜æ¢è¶‹åŠ¿æ¨¡å¼',
        'AVGPRICE': 'å¹³å‡ä»·æ ¼',
        'MEDPRICE': 'ä¸­ä½ä»·æ ¼',
        'TYPPRICE': 'å…¸å‹ä»·æ ¼',
        'WCLPRICE': 'åŠ æƒæ”¶ç›˜ä»·',
        'BETA': 'Betaç³»æ•°',
        'CORREL': 'ç›¸å…³ç³»æ•°',
        'LINEARREG': 'çº¿æ€§å›å½’',
        'LINEARREG_ANGLE': 'çº¿æ€§å›å½’è§’åº¦',
        'LINEARREG_INTERCEPT': 'çº¿æ€§å›å½’æˆªè·',
        'LINEARREG_SLOPE': 'çº¿æ€§å›å½’æ–œç‡',
        'STDDEV': 'æ ‡å‡†å·®',
        'TSF': 'æ—¶é—´åºåˆ—é¢„æµ‹',
        'VAR': 'æ–¹å·®',
        
        # èœ¡çƒ›å›¾å½¢æ€æŒ‡æ ‡
        'CDL2CROWS': 'ä¸¤åªä¹Œé¸¦',
        'CDL3BLACKCROWS': 'ä¸‰åªé»‘ä¹Œé¸¦',
        'CDL3INSIDE': 'ä¸‰å†…éƒ¨ä¸Šå‡å’Œä¸‹é™',
        'CDL3LINESTRIKE': 'ä¸‰çº¿æ‰“å‡»',
        'CDL3OUTSIDE': 'ä¸‰å¤–éƒ¨ä¸Šå‡å’Œä¸‹é™',
        'CDL3STARSINSOUTH': 'å—æ–¹ä¸‰æ˜Ÿ',
        'CDL3WHITESOLDIERS': 'ä¸‰ç™½å…µ',
        'CDLABANDONEDBABY': 'å¼ƒå©´',
        'CDLADVANCEBLOCK': 'å¤§æ•Œå½“å‰',
        'CDLBELTHOLD': 'æ‰è…°å¸¦çº¿',
        'CDLBREAKAWAY': 'è„±ç¦»',
        'CDLCLOSINGMARUBOZU': 'æ”¶ç›˜ç¼ºå½±çº¿',
        'CDLCONCEALBABYSWALL': 'è—å©´åæ²¡',
        'CDLCOUNTERATTACK': 'åå‡»çº¿',
        'CDLDARKCLOUDCOVER': 'ä¹Œäº‘å‹é¡¶',
        'CDLDOJI': 'åå­—',
        'CDLDOJISTAR': 'åå­—æ˜Ÿ',
        'CDLDRAGONFLYDOJI': 'èœ»èœ“åå­—',
        'CDLENGULFING': 'åå™¬æ¨¡å¼',
        'CDLEVENINGDOJISTAR': 'é»„æ˜åå­—æ˜Ÿ',
        'CDLEVENINGSTAR': 'é»„æ˜æ˜Ÿ',
        'CDLGAPSIDESIDEWHITE': 'å‘ä¸Š/ä¸‹è·³ç©ºå¹¶åˆ—é˜³çº¿',
        'CDLGRAVESTONEDOJI': 'å¢“ç¢‘åå­—',
        'CDLHAMMER': 'é”¤å¤´',
        'CDLHANGINGMAN': 'ä¸ŠåŠçº¿',
        'CDLHARAMI': 'æ¯å­çº¿',
        'CDLHARAMICROSS': 'åå­—å­•çº¿',
        'CDLHIGHWAVE': 'é•¿è…¿åå­—',
        'CDLHIKKAKE': 'Hikkakeæ¨¡å¼',
        'CDLHIKKAKEMOD': 'ä¿®æ­£Hikkakeæ¨¡å¼',
        'CDLHOMINGPIGEON': 'å®¶é¸½',
        'CDLIDENTICAL3CROWS': 'ä¸‰èƒèƒä¹Œé¸¦',
        'CDLINNECK': 'é¢ˆå†…çº¿',
        'CDLINVERTEDHAMMER': 'å€’é”¤å¤´',
        'CDLKICKING': 'åå†²',
        'CDLKICKINGBYLENGTH': 'ç”±è¾ƒé•¿ç¼ºå½±çº¿å†³å®šçš„åå†²',
        'CDLLADDERBOTTOM': 'æ¢¯åº•',
        'CDLLONGLEGGEDDOJI': 'é•¿è…¿åå­—',
        'CDLLONGLINE': 'é•¿èœ¡çƒ›',
        'CDLMARUBOZU': 'å…‰å¤´å…‰è„š',
        'CDLMATCHINGLOW': 'ç›¸åŒä½ä»·',
        'CDLMATHOLD': 'é“ºå«',
        'CDLMORNINGDOJISTAR': 'æ—©æ™¨åå­—æ˜Ÿ',
        'CDLMORNINGSTAR': 'æ—©æ™¨æ˜Ÿ',
        'CDLONNECK': 'é¢ˆä¸Šçº¿',
        'CDLPIERCING': 'åˆºé€',
        'CDLRICKSHAWMAN': 'é»„åŒ…è½¦å¤«',
        'CDLRISEFALL3METHODS': 'ä¸Šå‡/ä¸‹é™ä¸‰æ³•',
        'CDLSEPARATINGLINES': 'åˆ†ç¦»çº¿',
        'CDLSHOOTINGSTAR': 'å°„å‡»ä¹‹æ˜Ÿ',
        'CDLSHORTLINE': 'çŸ­èœ¡çƒ›',
        'CDLSPINNINGTOP': 'çººé”¤',
        'CDLSTALLEDPATTERN': 'åœé¡¿å½¢æ€',
        'CDLSTICKSANDWICH': 'æ¡å½¢ä¸‰æ˜æ²»',
        'CDLTAKURI': 'Takuriçº¿',
        'CDLTASUKIGAP': 'Tasukiè·³ç©º',
        'CDLTHRUSTING': 'æ’å…¥',
        'CDLTRISTAR': 'ä¸‰æ˜Ÿ',
        'CDLUNIQUE3RIVER': 'å¥‡ç‰¹ä¸‰æ²³åºŠ',
        'CDLUPSIDEGAP2CROWS': 'å‘ä¸Šè·³ç©ºçš„ä¸¤åªä¹Œé¸¦',
        'CDLXSIDEGAP3METHODS': 'Xä¾§è·³ç©ºä¸‰æ³•',
        
        # è´¢åŠ¡æŒ‡æ ‡
        'MarketCap': 'å¸‚å€¼',
        'PriceToBook': 'å¸‚å‡€ç‡',
        'PE': 'å¸‚ç›ˆç‡',
        'ROE': 'å‡€èµ„äº§æ”¶ç›Šç‡',
        'ROA': 'æ€»èµ„äº§æ”¶ç›Šç‡',
        'CurrentRatio': 'æµåŠ¨æ¯”ç‡',
        'QuickRatio': 'é€ŸåŠ¨æ¯”ç‡',
        'DebtToEquity': 'èµ„äº§è´Ÿå€ºç‡',
        'GrossMargin': 'æ¯›åˆ©ç‡',
        'NetMargin': 'å‡€åˆ©ç‡',
        'TobinsQ': 'æ‰˜å®¾Qå€¼',
        'turnover_5': '5æ—¥æ¢æ‰‹ç‡',
        'turnover_10': '10æ—¥æ¢æ‰‹ç‡',
        'turnover_20': '20æ—¥æ¢æ‰‹ç‡',
        'turnover_60': '60æ—¥æ¢æ‰‹ç‡',
        
        # æ³¢åŠ¨ç‡æŒ‡æ ‡
        'RealizedVolatility_5': '5æ—¥å·²å®ç°æ³¢åŠ¨ç‡',
        'RealizedVolatility_10': '10æ—¥å·²å®ç°æ³¢åŠ¨ç‡',
        'RealizedVolatility_20': '20æ—¥å·²å®ç°æ³¢åŠ¨ç‡',
        'RealizedVolatility_60': '60æ—¥å·²å®ç°æ³¢åŠ¨ç‡',
        'SemiDeviation_5': '5æ—¥åŠå˜å·®',
        'SemiDeviation_10': '10æ—¥åŠå˜å·®',
        'SemiDeviation_20': '20æ—¥åŠå˜å·®',
        'SemiDeviation_60': '60æ—¥åŠå˜å·®',
    }
    
    # ä¸ºAlpha360æŒ‡æ ‡ç”Ÿæˆæ ‡ç­¾
    @classmethod
    def _generate_alpha360_labels(cls):
        """ç”ŸæˆAlpha360æŒ‡æ ‡çš„ä¸­æ–‡æ ‡ç­¾"""
        labels = {}
        features = ['open', 'close', 'high', 'low', 'volume']
        feature_names = {'open': 'å¼€ç›˜ä»·', 'close': 'æ”¶ç›˜ä»·', 'high': 'æœ€é«˜ä»·', 'low': 'æœ€ä½ä»·', 'volume': 'æˆäº¤é‡'}
        
        for i in range(60):
            for feature in features:
                for j in range(6):
                    col_name = f"ALPHA360_{feature}_{i}_{j}"
                    labels[col_name] = f"{feature_names[feature]}æ ‡å‡†åŒ–T-{i}æœŸç¬¬{j}ç»´"
        
        return labels
    
    def get_field_labels(self, columns):
        """è·å–æ‰€æœ‰å­—æ®µçš„ä¸­æ–‡æ ‡ç­¾"""
        # åˆå¹¶é™æ€æ ‡ç­¾å’ŒåŠ¨æ€ç”Ÿæˆçš„Alpha360æ ‡ç­¾
        all_labels = self.FIELD_LABELS.copy()
        all_labels.update(self._generate_alpha360_labels())
        
        # ä¸ºæ¯ä¸ªåˆ—åç”Ÿæˆæ ‡ç­¾ï¼Œå¦‚æœæ²¡æœ‰é¢„å®šä¹‰æ ‡ç­¾åˆ™ä½¿ç”¨é»˜è®¤æ ‡ç­¾
        labels = []
        for col in columns:
            if col in all_labels:
                labels.append(all_labels[col])
            elif col.startswith('ALPHA360_'):
                # å¯¹äºæœªé¢„å®šä¹‰çš„Alpha360æŒ‡æ ‡ï¼Œç”Ÿæˆé»˜è®¤æ ‡ç­¾
                labels.append(f"Alpha360æŒ‡æ ‡_{col.replace('ALPHA360_', '')}")
            elif col.startswith('ALPHA158_'):
                # å¯¹äºæœªé¢„å®šä¹‰çš„Alpha158æŒ‡æ ‡ï¼Œç”Ÿæˆé»˜è®¤æ ‡ç­¾
                labels.append(f"Alpha158æŒ‡æ ‡_{col.replace('ALPHA158_', '')}")
            elif col.startswith('CDL'):
                # å¯¹äºæœªé¢„å®šä¹‰çš„èœ¡çƒ›å›¾å½¢æ€ï¼Œç”Ÿæˆé»˜è®¤æ ‡ç­¾
                labels.append(f"èœ¡çƒ›å›¾å½¢æ€_{col.replace('CDL', '')}")
            else:
                # å…¶ä»–æœªé¢„å®šä¹‰çš„æŒ‡æ ‡ï¼Œä½¿ç”¨åŸå­—æ®µå
                labels.append(col)
        
        return labels
    
    def __init__(self, data_dir: str = r"D:\stk_data\trd\us_data", financial_data_dir: str = None, 
                 max_workers: int = None, enable_parallel: bool = True):
        self.data_dir = Path(data_dir)
        self.features_dir = self.data_dir / "features"
        self.output_dir = self.data_dir
        
        # å¤šçº¿ç¨‹é…ç½®
        self.enable_parallel = enable_parallel
        self.max_workers = max_workers or min(32, (multiprocessing.cpu_count() or 1) + 4)
        
        # è´¢åŠ¡æ•°æ®ç›®å½•
        if financial_data_dir:
            self.financial_data_dir = Path(financial_data_dir)
        else:
            # é»˜è®¤ä»ç”¨æˆ·çš„è´¢åŠ¡æ•°æ®ç›®å½•æŸ¥æ‰¾
            self.financial_data_dir = Path.home() / ".qlib" / "financial_data"
        
        logger.info(f"æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"è´¢åŠ¡æ•°æ®ç›®å½•: {self.financial_data_dir}")
        logger.info(f"å¤šçº¿ç¨‹é…ç½®: {'å¯ç”¨' if self.enable_parallel else 'ç¦ç”¨'} (æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers})")
        
        if not self.data_dir.exists():
            logger.error(f"Data directory does not exist: {self.data_dir}")
            raise FileNotFoundError(f"Data directory does not exist: {self.data_dir}")
        
        if not self.features_dir.exists():
            logger.warning(f"Features directory does not exist: {self.features_dir}")
        
        # åˆå§‹åŒ–è´¢åŠ¡æ•°æ®ç¼“å­˜
        self.financial_cache = {}
        self._load_financial_data()
        
        # çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
        self._local = threading.local()
        
        # çº¿ç¨‹é”ï¼Œä¿æŠ¤å…±äº«èµ„æº
        self._lock = threading.Lock()
    
    def _load_financial_data(self):
        """åŠ è½½è´¢åŠ¡æ•°æ®åˆ°å†…å­˜ç¼“å­˜"""
        try:
            if self.financial_data_dir is None:
                logger.warning("æœªæŒ‡å®šè´¢åŠ¡æ•°æ®ç›®å½•ï¼Œå°†ä½¿ç”¨ä¼°ç®—å€¼")
                return
                
            logger.info(f"æ­£åœ¨åŠ è½½è´¢åŠ¡æ•°æ®ä»: {self.financial_data_dir}")
            
            # åŠ è½½å„ç§è´¢åŠ¡æ•°æ®
            data_types = ['info', 'financials', 'balance_sheet', 'cashflow', 'dividends', 'financial_ratios']
            
            for data_type in data_types:
                data_path = self.financial_data_dir / data_type
                if data_path.exists():
                    self.financial_cache[data_type] = {}
                    
                    # è¯»å–CSVæ–‡ä»¶
                    csv_files = list(data_path.glob("*.csv"))
                    if csv_files:
                        for csv_file in csv_files:
                            symbol = csv_file.stem.upper()
                            try:
                                df = pd.read_csv(csv_file, index_col=0)
                                self.financial_cache[data_type][symbol] = df
                            except Exception as e:
                                logger.warning(f"Failed to load {data_type} for {symbol}: {e}")
                        
                        logger.info(f"âœ… åŠ è½½ {data_type} æ•°æ®: {len(self.financial_cache[data_type])} åªè‚¡ç¥¨")
                    else:
                        logger.warning(f"ğŸ“ {data_type} ç›®å½•ä¸ºç©º")
                else:
                    logger.warning(f"ğŸ“ è´¢åŠ¡æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
                    
        except Exception as e:
            logger.error(f"åŠ è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            self.financial_cache = {}
    
    def read_qlib_binary_data(self, symbol: str) -> Optional[pd.DataFrame]:
        """è¯»å–QlibäºŒè¿›åˆ¶æ•°æ®"""
        symbol_dir = self.features_dir / symbol.lower()
        
        if not symbol_dir.exists():
            return None
        
        features = ['open', 'high', 'low', 'close', 'volume']
        data_dict = {}
        
        try:
            for feature in features:
                bin_file = symbol_dir / f"{feature}.day.bin"
                if bin_file.exists():
                    with open(bin_file, 'rb') as f:
                        values = []
                        while True:
                            data = f.read(4)
                            if not data:
                                break
                            value = struct.unpack('<f', data)[0]
                            if not np.isnan(value) and not np.isinf(value):
                                values.append(value)
                            else:
                                values.append(0.0)
                        data_dict[feature.title()] = values
            
            if not data_dict:
                return None
            
            # Ensure all arrays have the same length
            lengths = [len(v) for v in data_dict.values()]
            if len(set(lengths)) > 1:
                min_length = min(lengths)
                for key in data_dict:
                    data_dict[key] = data_dict[key][:min_length]
            
            df = pd.DataFrame(data_dict)
            
            # Read actual calendar dates from qlib
            calendar_file = self.data_dir / "calendars" / "day.txt"
            if calendar_file.exists():
                with open(calendar_file, 'r') as f:
                    calendar_dates = [line.strip() for line in f.readlines()]
                calendar_dates = [pd.to_datetime(date) for date in calendar_dates if date.strip()]
                
                # The data in qlib is typically aligned with the calendar in reverse order
                if len(df) <= len(calendar_dates):
                    dates = calendar_dates[-len(df):]
                else:
                    # If data is longer than calendar, extend backwards
                    latest_date = calendar_dates[-1] if calendar_dates else pd.to_datetime('2025-06-27')
                    all_dates = pd.bdate_range(end=latest_date, periods=len(df), freq='B')
                    dates = all_dates.tolist()
            else:
                # Fallback: generate business days ending at a reasonable date
                end_date = pd.to_datetime('2025-06-27')
                dates = pd.bdate_range(end=end_date, periods=len(df), freq='B')
            
            df.index = pd.DatetimeIndex(dates)
            
            return df
            
        except Exception as e:
            logger.warning(f"Failed to read binary data {symbol}: {e}")
            return None
    
    def get_available_stocks(self) -> List[str]:
        """è·å–å¯ç”¨è‚¡ç¥¨åˆ—è¡¨"""
        stocks = []
        
        if not self.features_dir.exists():
            logger.error(f"Features directory does not exist: {self.features_dir}")
            return stocks
        
        for stock_dir in self.features_dir.iterdir():
            if stock_dir.is_dir():
                required_files = ['open.day.bin', 'high.day.bin', 'low.day.bin', 'close.day.bin', 'volume.day.bin']
                if all((stock_dir / file).exists() for file in required_files):
                    symbol = stock_dir.name.upper()
                    stocks.append(symbol)
        
        logger.info(f"Found {len(stocks)} stocks data")
        return sorted(stocks)
    
    def get_financial_data(self, symbol: str, data_type: str) -> Optional[pd.DataFrame]:
        """è·å–è´¢åŠ¡æ•°æ®"""
        try:
            # å°è¯•å¤šç§ç¬¦å·æ ¼å¼
            symbol_variants = [
                symbol,                        # åŸå§‹ç¬¦å·
                symbol.replace('_', '.'),      # ä¸‹åˆ’çº¿è½¬ç‚¹å· (0002_HK -> 0002.HK)
                symbol.replace('.', '_'),      # ç‚¹å·è½¬ä¸‹åˆ’çº¿ (0002.HK -> 0002_HK)
                symbol.upper(),                # å¤§å†™
                symbol.replace('_HK', '.HK'),  # ç‰¹å®šè½¬æ¢
                symbol.replace('.HK', '_HK')   # ç‰¹å®šè½¬æ¢
            ]
            
            if data_type in self.financial_cache:
                for variant in symbol_variants:
                    if variant in self.financial_cache[data_type]:
                        return self.financial_cache[data_type][variant]
            return None
        except Exception as e:
            logger.warning(f"Failed to get financial data for {symbol}, {data_type}: {e}")
            return None
    
    def _safe_divide(self, a, b, fill_value=0.0):
        """å®‰å…¨é™¤æ³•æ“ä½œï¼Œé¿å…é™¤é›¶é”™è¯¯"""
        return np.where(np.abs(b) > 1e-12, a / b, fill_value)
    
    def _get_calculated_indicators(self):
        """è·å–çº¿ç¨‹æœ¬åœ°çš„æŒ‡æ ‡é›†åˆ"""
        if not hasattr(self._local, 'calculated_indicators'):
            self._local.calculated_indicators = set()
        return self._local.calculated_indicators
    
    def _add_indicator(self, indicators: dict, name: str, values):
        """æ·»åŠ æŒ‡æ ‡åˆ°å­—å…¸ä¸­ï¼Œé¿å…é‡å¤ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        calculated_indicators = self._get_calculated_indicators()
        if name not in calculated_indicators:
            indicators[name] = values
            calculated_indicators.add(name)
        else:
            logger.debug(f"æŒ‡æ ‡ {name} å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤è®¡ç®—")
    
    def _reset_indicators_cache(self):
        """é‡ç½®çº¿ç¨‹æœ¬åœ°çš„æŒ‡æ ‡ç¼“å­˜"""
        if hasattr(self._local, 'calculated_indicators'):
            self._local.calculated_indicators.clear()
    
    def calculate_all_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ï¼ˆå…±çº¦60ä¸ªï¼‰"""
        if data.empty or len(data) < 50:
            logger.warning("Insufficient data for calculating technical indicators")
            return pd.DataFrame()
        
        try:
            indicators = {}
            
            # Clean data and convert to numpy arrays for talib
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            volume = data['Volume'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0).values
            
            # 1. Moving Averages (ç§»åŠ¨å¹³å‡çº¿ç±») - 12ä¸ª
            self._add_indicator(indicators, 'SMA_5', talib.SMA(close, timeperiod=5))
            self._add_indicator(indicators, 'SMA_10', talib.SMA(close, timeperiod=10))
            self._add_indicator(indicators, 'SMA_20', talib.SMA(close, timeperiod=20))
            self._add_indicator(indicators, 'SMA_50', talib.SMA(close, timeperiod=50))
            
            self._add_indicator(indicators, 'EMA_5', talib.EMA(close, timeperiod=5))
            self._add_indicator(indicators, 'EMA_10', talib.EMA(close, timeperiod=10))
            self._add_indicator(indicators, 'EMA_20', talib.EMA(close, timeperiod=20))
            self._add_indicator(indicators, 'EMA_50', talib.EMA(close, timeperiod=50))
            
            self._add_indicator(indicators, 'DEMA_20', talib.DEMA(close, timeperiod=20))
            self._add_indicator(indicators, 'TEMA_20', talib.TEMA(close, timeperiod=20))
            self._add_indicator(indicators, 'KAMA_30', talib.KAMA(close, timeperiod=30))
            self._add_indicator(indicators, 'WMA_20', talib.WMA(close, timeperiod=20))
            
            # 2. MACD Family - 3ä¸ª
            indicators['MACD'], indicators['MACD_Signal'], indicators['MACD_Histogram'] = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)
            indicators['MACDEXT'], _, _ = talib.MACDEXT(close, fastperiod=12, slowperiod=26, signalperiod=9)
            indicators['MACDFIX'], _, _ = talib.MACDFIX(close, signalperiod=9)
            
            # 3. Momentum Oscillators (åŠ¨é‡æŒ¯è¡å™¨) - 6ä¸ª
            indicators['RSI_14'] = talib.RSI(close, timeperiod=14)
            indicators['CCI_14'] = talib.CCI(high, low, close, timeperiod=14)
            indicators['CMO_14'] = talib.CMO(close, timeperiod=14)
            indicators['MFI_14'] = talib.MFI(high, low, close, volume, timeperiod=14)
            indicators['WILLR_14'] = talib.WILLR(high, low, close, timeperiod=14)
            indicators['ULTOSC'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)
            
            # 4. Trend Indicators (è¶‹åŠ¿æŒ‡æ ‡) - 13ä¸ª
            indicators['ADX_14'] = talib.ADX(high, low, close, timeperiod=14)
            indicators['ADXR_14'] = talib.ADXR(high, low, close, timeperiod=14)
            indicators['APO'] = talib.APO(close, fastperiod=12, slowperiod=26)
            indicators['AROON_DOWN'], indicators['AROON_UP'] = talib.AROON(high, low, timeperiod=14)
            indicators['AROONOSC_14'] = talib.AROONOSC(high, low, timeperiod=14)
            indicators['BOP'] = talib.BOP(open_price, high, low, close)
            indicators['DX_14'] = talib.DX(high, low, close, timeperiod=14)
            indicators['MINUS_DI_14'] = talib.MINUS_DI(high, low, close, timeperiod=14)
            indicators['MINUS_DM_14'] = talib.MINUS_DM(high, low, timeperiod=14)
            indicators['PLUS_DI_14'] = talib.PLUS_DI(high, low, close, timeperiod=14)
            indicators['PLUS_DM_14'] = talib.PLUS_DM(high, low, timeperiod=14)
            indicators['PPO'] = talib.PPO(close, fastperiod=12, slowperiod=26)
            indicators['TRIX_30'] = talib.TRIX(close, timeperiod=30)
            
            # 5. Momentum Indicators (åŠ¨é‡æŒ‡æ ‡) - 5ä¸ª
            indicators['MOM_10'] = talib.MOM(close, timeperiod=10)
            indicators['ROC_10'] = talib.ROC(close, timeperiod=10)
            indicators['ROCP_10'] = talib.ROCP(close, timeperiod=10)
            indicators['ROCR_10'] = talib.ROCR(close, timeperiod=10)
            indicators['ROCR100_10'] = talib.ROCR100(close, timeperiod=10)
            
            # 6. Bollinger Bands - 3ä¸ª
            indicators['BB_Upper'], indicators['BB_Middle'], indicators['BB_Lower'] = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)
            
            # 7. Stochastic (éšæœºæŒ‡æ ‡) - 6ä¸ª
            indicators['STOCH_K'], indicators['STOCH_D'] = talib.STOCH(high, low, close, fastk_period=14, slowk_period=3, slowd_period=3)
            indicators['STOCHF_K'], indicators['STOCHF_D'] = talib.STOCHF(high, low, close, fastk_period=14, fastd_period=3)
            indicators['STOCHRSI_K'], indicators['STOCHRSI_D'] = talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3)
            
            # 8. Volatility Indicators (æ³¢åŠ¨ç‡æŒ‡æ ‡) - 3ä¸ª
            indicators['ATR_14'] = talib.ATR(high, low, close, timeperiod=14)
            indicators['NATR_14'] = talib.NATR(high, low, close, timeperiod=14)
            indicators['TRANGE'] = talib.TRANGE(high, low, close)
            
            # 9. Volume Indicators (æˆäº¤é‡æŒ‡æ ‡) - 3ä¸ª
            indicators['OBV'] = talib.OBV(close, volume)
            indicators['AD'] = talib.AD(high, low, close, volume)
            indicators['ADOSC'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)
            
            # 10. Hilbert Transform (å¸Œå°”ä¼¯ç‰¹å˜æ¢) - 7ä¸ª
            indicators['HT_DCPERIOD'] = talib.HT_DCPERIOD(close)
            indicators['HT_DCPHASE'] = talib.HT_DCPHASE(close)
            indicators['HT_INPHASE'], indicators['HT_QUADRATURE'] = talib.HT_PHASOR(close)
            indicators['HT_SINE'], indicators['HT_LEADSINE'] = talib.HT_SINE(close)
            indicators['HT_TRENDMODE'] = talib.HT_TRENDMODE(close)
            indicators['HT_TRENDLINE'] = talib.HT_TRENDLINE(close)
            
            # 11. Math Transform (æ•°å­¦å˜æ¢) - 8ä¸ª
            indicators['AVGPRICE'] = talib.AVGPRICE(open_price, high, low, close)
            indicators['MEDPRICE'] = talib.MEDPRICE(high, low)
            indicators['TYPPRICE'] = talib.TYPPRICE(high, low, close)
            indicators['WCLPRICE'] = talib.WCLPRICE(high, low, close)
            indicators['MIDPOINT'] = talib.MIDPOINT(close, timeperiod=14)
            indicators['MIDPRICE'] = talib.MIDPRICE(high, low, timeperiod=14)
            indicators['MAMA'], indicators['FAMA'] = talib.MAMA(close)
            
            # 12. Statistical Functions (ç»Ÿè®¡å‡½æ•°) - 7ä¸ª
            indicators['LINEARREG'] = talib.LINEARREG(close, timeperiod=14)
            indicators['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(close, timeperiod=14)
            indicators['LINEARREG_INTERCEPT'] = talib.LINEARREG_INTERCEPT(close, timeperiod=14)
            indicators['LINEARREG_SLOPE'] = talib.LINEARREG_SLOPE(close, timeperiod=14)
            indicators['STDDEV'] = talib.STDDEV(close, timeperiod=30)
            indicators['TSF'] = talib.TSF(close, timeperiod=14)
            indicators['VAR'] = talib.VAR(close, timeperiod=30)
            
            # 13. Min/Max Functions - 2ä¸ª
            indicators['MAXINDEX'] = talib.MAXINDEX(close, timeperiod=30)
            indicators['MININDEX'] = talib.MININDEX(close, timeperiod=30)
            
            # è½¬æ¢ä¸ºDataFrame
            indicators_df = pd.DataFrame(indicators, index=data.index)
            
            logger.info(f"è®¡ç®—äº† {len(indicators)} ä¸ªæŠ€æœ¯æŒ‡æ ‡")
            return indicators_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_alpha158_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—Alpha158æŒ‡æ ‡ä½“ç³» (158ä¸ªæŒ‡æ ‡)
        åŒ…æ‹¬KBARæŒ‡æ ‡ã€ä»·æ ¼æŒ‡æ ‡ã€æˆäº¤é‡æŒ‡æ ‡ã€æ»šåŠ¨æŠ€æœ¯æŒ‡æ ‡
        """
        if data.empty or len(data) < 60:
            logger.warning("æ•°æ®ä¸è¶³ä»¥è®¡ç®—Alpha158æŒ‡æ ‡")
            return pd.DataFrame()
        
        try:
            indicators = {}
            
            # æ¸…ç†æ•°æ®
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            volume = data['Volume'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0).values
            
            # è®¡ç®—VWAP
            vwap = self._safe_divide(
                np.convolve(close * volume, np.ones(1), 'same'),
                np.convolve(volume, np.ones(1), 'same')
            )
            
            # 1. KBARæŒ‡æ ‡ (9ä¸ª)
            self._add_indicator(indicators, 'ALPHA158_KMID', self._safe_divide(close - open_price, open_price))
            self._add_indicator(indicators, 'ALPHA158_KLEN', self._safe_divide(high - low, open_price))
            self._add_indicator(indicators, 'ALPHA158_KMID2', self._safe_divide(close - open_price, high - low + 1e-12))
            self._add_indicator(indicators, 'ALPHA158_KUP', self._safe_divide(high - np.maximum(open_price, close), open_price))
            self._add_indicator(indicators, 'ALPHA158_KUP2', self._safe_divide(high - np.maximum(open_price, close), high - low + 1e-12))
            self._add_indicator(indicators, 'ALPHA158_KLOW', self._safe_divide(np.minimum(open_price, close) - low, open_price))
            self._add_indicator(indicators, 'ALPHA158_KLOW2', self._safe_divide(np.minimum(open_price, close) - low, high - low + 1e-12))
            self._add_indicator(indicators, 'ALPHA158_KSFT', self._safe_divide(2 * close - high - low, open_price))
            self._add_indicator(indicators, 'ALPHA158_KSFT2', self._safe_divide(2 * close - high - low, high - low + 1e-12))
            
            # 2. ä»·æ ¼æŒ‡æ ‡ (æ ‡å‡†åŒ–åˆ°æ”¶ç›˜ä»·)
            features = ['OPEN', 'HIGH', 'LOW', 'VWAP']
            for feature in features:
                if feature == 'OPEN':
                    price_values = open_price
                elif feature == 'HIGH':
                    price_values = high
                elif feature == 'LOW':
                    price_values = low
                elif feature == 'VWAP':
                    price_values = vwap
                
                self._add_indicator(indicators, f'ALPHA158_{feature}0', self._safe_divide(price_values, close))
            
            # 3. æˆäº¤é‡æŒ‡æ ‡
            self._add_indicator(indicators, 'ALPHA158_VOLUME0', self._safe_divide(volume, volume + 1e-12))
            
            # 4. æ»šåŠ¨æŠ€æœ¯æŒ‡æ ‡
            windows = [5, 10, 20, 30, 60]
            
            # ROC - Rate of Change
            for d in windows:
                ref_close = np.roll(close, d)
                self._add_indicator(indicators, f'ALPHA158_ROC{d}', self._safe_divide(ref_close, close))
            
            # MA - Simple Moving Average
            for d in windows:
                ma_values = pd.Series(close).rolling(window=d, min_periods=1).mean().values
                self._add_indicator(indicators, f'ALPHA158_MA{d}', self._safe_divide(ma_values, close))
            
            # STD - Standard Deviation
            for d in windows:
                std_values = pd.Series(close).rolling(window=d, min_periods=1).std().fillna(0).values
                self._add_indicator(indicators, f'ALPHA158_STD{d}', self._safe_divide(std_values, close))
            
            # BETA - Slope
            for d in windows:
                beta_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    x = np.arange(d)
                    y = close[i-d+1:i+1]
                    if len(y) > 1:
                        try:
                            slope = np.polyfit(x, y, 1)[0]
                            beta_values[i] = slope
                        except:
                            beta_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_BETA{d}', self._safe_divide(beta_values, close))
            
            # RSQR - R-square
            for d in windows:
                rsqr_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    x = np.arange(d)
                    y = close[i-d+1:i+1]
                    if len(y) > 1:
                        try:
                            correlation_matrix = np.corrcoef(x, y)
                            rsqr_values[i] = correlation_matrix[0, 1] ** 2 if not np.isnan(correlation_matrix[0, 1]) else 0
                        except:
                            rsqr_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_RSQR{d}', rsqr_values)
            
            # MAX/MIN
            for d in windows:
                max_values = pd.Series(high).rolling(window=d, min_periods=1).max().values
                min_values = pd.Series(low).rolling(window=d, min_periods=1).min().values
                self._add_indicator(indicators, f'ALPHA158_MAX{d}', self._safe_divide(max_values, close))
                self._add_indicator(indicators, f'ALPHA158_MIN{d}', self._safe_divide(min_values, close))
            
            # QTLU/QTLD - Quantiles
            for d in windows:
                qtlu_values = pd.Series(close).rolling(window=d, min_periods=1).quantile(0.8).values
                qtld_values = pd.Series(close).rolling(window=d, min_periods=1).quantile(0.2).values
                self._add_indicator(indicators, f'ALPHA158_QTLU{d}', self._safe_divide(qtlu_values, close))
                self._add_indicator(indicators, f'ALPHA158_QTLD{d}', self._safe_divide(qtld_values, close))
            
            # RANK - Percentile rank
            for d in windows:
                rank_values = pd.Series(close).rolling(window=d, min_periods=1).rank(pct=True).values
                self._add_indicator(indicators, f'ALPHA158_RANK{d}', rank_values)
            
            # RSV - Relative Strength Value
            for d in windows:
                min_low = pd.Series(low).rolling(window=d, min_periods=1).min().values
                max_high = pd.Series(high).rolling(window=d, min_periods=1).max().values
                self._add_indicator(indicators, f'ALPHA158_RSV{d}', self._safe_divide(close - min_low, max_high - min_low + 1e-12))
            
            # RESI - Linear Regression Residual
            for d in windows:
                resi_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    x = np.arange(d)
                    y = close[i-d+1:i+1]
                    if len(y) > 1:
                        try:
                            slope, intercept = np.polyfit(x, y, 1)
                            predicted = slope * (d-1) + intercept
                            resi_values[i] = y[-1] - predicted
                        except:
                            resi_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_RESI{d}', self._safe_divide(resi_values, close))
            
            # IMAX - Index of Maximum
            for d in windows:
                imax_values = np.zeros_like(close)
                for i in range(d, len(high)):
                    window_high = high[i-d+1:i+1]
                    if len(window_high) > 0:
                        imax_values[i] = (len(window_high) - 1 - np.argmax(window_high)) / d
                self._add_indicator(indicators, f'ALPHA158_IMAX{d}', imax_values)
            
            # IMIN - Index of Minimum  
            for d in windows:
                imin_values = np.zeros_like(close)
                for i in range(d, len(low)):
                    window_low = low[i-d+1:i+1]
                    if len(window_low) > 0:
                        imin_values[i] = (len(window_low) - 1 - np.argmin(window_low)) / d
                self._add_indicator(indicators, f'ALPHA158_IMIN{d}', imin_values)
            
            # IMXD - Index Max - Index Min Difference
            for d in windows:
                imxd_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_high = high[i-d+1:i+1] 
                    window_low = low[i-d+1:i+1]
                    if len(window_high) > 0 and len(window_low) > 0:
                        idx_max = len(window_high) - 1 - np.argmax(window_high)
                        idx_min = len(window_low) - 1 - np.argmin(window_low)
                        imxd_values[i] = (idx_max - idx_min) / d
                self._add_indicator(indicators, f'ALPHA158_IMXD{d}', imxd_values)
            
            # CORR - Correlation between close and log(volume)
            for d in windows:
                corr_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_close = close[i-d+1:i+1]
                    window_volume = volume[i-d+1:i+1]
                    if len(window_close) > 1:
                        try:
                            log_volume = np.log(window_volume + 1)
                            if np.std(window_close) > 1e-8 and np.std(log_volume) > 1e-8:
                                corr_values[i] = np.corrcoef(window_close, log_volume)[0, 1]
                        except:
                            corr_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_CORR{d}', corr_values)
            
            # CORD - Correlation between price change and volume change
            for d in windows:
                cord_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    if i >= d:
                        close_change = close[i-d+2:i+1] / close[i-d+1:i]
                        volume_change = np.log((volume[i-d+2:i+1] / (volume[i-d+1:i] + 1e-12)) + 1)
                        if len(close_change) > 1:
                            try:
                                if np.std(close_change) > 1e-8 and np.std(volume_change) > 1e-8:
                                    cord_values[i] = np.corrcoef(close_change, volume_change)[0, 1]
                            except:
                                cord_values[i] = 0
                self._add_indicator(indicators, f'ALPHA158_CORD{d}', cord_values)
            
            # CNTP - Count of Positive returns
            for d in windows:
                cntp_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_returns = close[i-d+2:i+1] > close[i-d+1:i]
                    if len(window_returns) > 0:
                        cntp_values[i] = np.mean(window_returns)
                self._add_indicator(indicators, f'ALPHA158_CNTP{d}', cntp_values)
            
            # CNTN - Count of Negative returns
            for d in windows:
                cntn_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_returns = close[i-d+2:i+1] < close[i-d+1:i]
                    if len(window_returns) > 0:
                        cntn_values[i] = np.mean(window_returns)
                self._add_indicator(indicators, f'ALPHA158_CNTN{d}', cntn_values)
            
            # CNTD - Count Difference (CNTP - CNTN)
            for d in windows:
                cntd_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    window_pos = close[i-d+2:i+1] > close[i-d+1:i]
                    window_neg = close[i-d+2:i+1] < close[i-d+1:i]
                    if len(window_pos) > 0:
                        cntd_values[i] = np.mean(window_pos) - np.mean(window_neg)
                self._add_indicator(indicators, f'ALPHA158_CNTD{d}', cntd_values)
            
            # SUMP - Sum of Positive returns ratio
            for d in windows:
                sump_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    changes = close[i-d+2:i+1] - close[i-d+1:i]
                    if len(changes) > 0:
                        positive_sum = np.sum(np.maximum(changes, 0))
                        total_abs_sum = np.sum(np.abs(changes))
                        sump_values[i] = self._safe_divide(positive_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_SUMP{d}', sump_values)
            
            # SUMN - Sum of Negative returns ratio  
            for d in windows:
                sumn_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    changes = close[i-d+2:i+1] - close[i-d+1:i]
                    if len(changes) > 0:
                        negative_sum = np.sum(np.maximum(-changes, 0))
                        total_abs_sum = np.sum(np.abs(changes))
                        sumn_values[i] = self._safe_divide(negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_SUMN{d}', sumn_values)
            
            # SUMD - Sum Difference (SUMP - SUMN)
            for d in windows:
                sumd_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    changes = close[i-d+2:i+1] - close[i-d+1:i]
                    if len(changes) > 0:
                        positive_sum = np.sum(np.maximum(changes, 0))
                        negative_sum = np.sum(np.maximum(-changes, 0))
                        total_abs_sum = np.sum(np.abs(changes))
                        sumd_values[i] = self._safe_divide(positive_sum - negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_SUMD{d}', sumd_values)
            
            # VMA - Volume Moving Average
            for d in windows:
                vma_values = pd.Series(volume).rolling(window=d, min_periods=1).mean().values
                self._add_indicator(indicators, f'ALPHA158_VMA{d}', self._safe_divide(vma_values, volume + 1e-12))
            
            # VSTD - Volume Standard Deviation
            for d in windows:
                vstd_values = pd.Series(volume).rolling(window=d, min_periods=1).std().fillna(0).values
                self._add_indicator(indicators, f'ALPHA158_VSTD{d}', self._safe_divide(vstd_values, volume + 1e-12))
            
            # WVMA - Weighted Volume Moving Average (price change volatility weighted by volume)
            for d in windows:
                wvma_values = np.zeros_like(close)
                for i in range(d, len(close)):
                    price_changes = np.abs(close[i-d+2:i+1] / close[i-d+1:i] - 1)
                    weights = volume[i-d+2:i+1]
                    if len(price_changes) > 0:
                        weighted_changes = price_changes * weights
                        mean_weighted = np.mean(weighted_changes)
                        std_weighted = np.std(weighted_changes)
                        wvma_values[i] = self._safe_divide(std_weighted, mean_weighted + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_WVMA{d}', wvma_values)
            
            # VSUMP - Volume Sum Positive ratio
            for d in windows:
                vsump_values = np.zeros_like(close)
                for i in range(d, len(volume)):
                    vol_changes = volume[i-d+2:i+1] - volume[i-d+1:i] 
                    if len(vol_changes) > 0:
                        positive_sum = np.sum(np.maximum(vol_changes, 0))
                        total_abs_sum = np.sum(np.abs(vol_changes))
                        vsump_values[i] = self._safe_divide(positive_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_VSUMP{d}', vsump_values)
            
            # VSUMN - Volume Sum Negative ratio
            for d in windows:
                vsumn_values = np.zeros_like(close)
                for i in range(d, len(volume)):
                    vol_changes = volume[i-d+2:i+1] - volume[i-d+1:i]
                    if len(vol_changes) > 0:
                        negative_sum = np.sum(np.maximum(-vol_changes, 0))
                        total_abs_sum = np.sum(np.abs(vol_changes))
                        vsumn_values[i] = self._safe_divide(negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_VSUMN{d}', vsumn_values)
            
            # VSUMD - Volume Sum Difference (VSUMP - VSUMN)
            for d in windows:
                vsumd_values = np.zeros_like(close)
                for i in range(d, len(volume)):
                    vol_changes = volume[i-d+2:i+1] - volume[i-d+1:i]
                    if len(vol_changes) > 0:
                        positive_sum = np.sum(np.maximum(vol_changes, 0))
                        negative_sum = np.sum(np.maximum(-vol_changes, 0))
                        total_abs_sum = np.sum(np.abs(vol_changes))
                        vsumd_values[i] = self._safe_divide(positive_sum - negative_sum, total_abs_sum + 1e-12)
                self._add_indicator(indicators, f'ALPHA158_VSUMD{d}', vsumd_values)
            
            # è½¬æ¢ä¸ºDataFrame
            indicators_df = pd.DataFrame(indicators, index=data.index)
            
            logger.info(f"è®¡ç®—äº†Alpha158æŒ‡æ ‡ä½“ç³»: {len(indicators)} ä¸ªæŒ‡æ ‡")
            return indicators_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—Alpha158æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_alpha360_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—Alpha360æŒ‡æ ‡ä½“ç³» (360ä¸ªæŒ‡æ ‡)
        åŒ…æ‹¬è¿‡å»60å¤©çš„æ ‡å‡†åŒ–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        """
        if data.empty or len(data) < 60:
            logger.warning("æ•°æ®ä¸è¶³ä»¥è®¡ç®—Alpha360æŒ‡æ ‡")
            return pd.DataFrame()
        
        try:
            indicators = {}
            
            # æ¸…ç†æ•°æ®
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            volume = data['Volume'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0).values
            
            # è®¡ç®—VWAP
            vwap = self._safe_divide(
                np.convolve(close * volume, np.ones(1), 'same'),
                np.convolve(volume, np.ones(1), 'same')
            )
            
            # Alpha360: è¿‡å»60å¤©çš„ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®ï¼Œé™¤ä»¥å½“å‰æ”¶ç›˜ä»·æ ‡å‡†åŒ–
            # 1. CLOSE æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_CLOSE{i}', self._safe_divide(close, close))
                else:
                    ref_close = np.roll(close, i)
                    self._add_indicator(indicators, f'ALPHA360_CLOSE{i}', self._safe_divide(ref_close, close))
            
            # 2. OPEN æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_OPEN{i}', self._safe_divide(open_price, close))
                else:
                    ref_open = np.roll(open_price, i)
                    self._add_indicator(indicators, f'ALPHA360_OPEN{i}', self._safe_divide(ref_open, close))
            
            # 3. HIGH æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_HIGH{i}', self._safe_divide(high, close))
                else:
                    ref_high = np.roll(high, i)
                    self._add_indicator(indicators, f'ALPHA360_HIGH{i}', self._safe_divide(ref_high, close))
            
            # 4. LOW æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_LOW{i}', self._safe_divide(low, close))
                else:
                    ref_low = np.roll(low, i)
                    self._add_indicator(indicators, f'ALPHA360_LOW{i}', self._safe_divide(ref_low, close))
            
            # 5. VWAP æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_VWAP{i}', self._safe_divide(vwap, close))
                else:
                    ref_vwap = np.roll(vwap, i)
                    self._add_indicator(indicators, f'ALPHA360_VWAP{i}', self._safe_divide(ref_vwap, close))
            
            # 6. VOLUME æŒ‡æ ‡ (60ä¸ª)
            for i in range(59, -1, -1):
                if i == 0:
                    self._add_indicator(indicators, f'ALPHA360_VOLUME{i}', self._safe_divide(volume, volume + 1e-12))
                else:
                    ref_volume = np.roll(volume, i)
                    self._add_indicator(indicators, f'ALPHA360_VOLUME{i}', self._safe_divide(ref_volume, volume + 1e-12))
            
            # è½¬æ¢ä¸ºDataFrame
            indicators_df = pd.DataFrame(indicators, index=data.index)
            
            logger.info(f"è®¡ç®—äº†Alpha360æŒ‡æ ‡ä½“ç³»: {len(indicators)} ä¸ªæŒ‡æ ‡")
            return indicators_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—Alpha360æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_candlestick_patterns(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—èœ¡çƒ›å›¾å½¢æ€æŒ‡æ ‡ï¼ˆå…±61ä¸ªï¼‰"""
        if data.empty or len(data) < 10:
            logger.warning("Insufficient data for calculating candlestick patterns")
            return pd.DataFrame()
        
        try:
            patterns = {}
            
            # Clean data
            open_price = data['Open'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            high = data['High'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            low = data['Low'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            close = data['Close'].astype(float).replace([np.inf, -np.inf], np.nan).fillna(method='ffill').values
            
            # æ‰€æœ‰61ä¸ªèœ¡çƒ›å›¾å½¢æ€
            candle_patterns = [
                'CDL2CROWS', 'CDL3BLACKCROWS', 'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE',
                'CDL3STARSINSOUTH', 'CDL3WHITESOLDIERS', 'CDLABANDONEDBABY', 'CDLADVANCEBLOCK',
                'CDLBELTHOLD', 'CDLBREAKAWAY', 'CDLCLOSINGMARUBOZU', 'CDLCONCEALBABYSWALL',
                'CDLCOUNTERATTACK', 'CDLDARKCLOUDCOVER', 'CDLDOJI', 'CDLDOJISTAR', 'CDLDRAGONFLYDOJI',
                'CDLENGULFING', 'CDLEVENINGDOJISTAR', 'CDLEVENINGSTAR', 'CDLGAPSIDESIDEWHITE',
                'CDLGRAVESTONEDOJI', 'CDLHAMMER', 'CDLHANGINGMAN', 'CDLHARAMI', 'CDLHARAMICROSS',
                'CDLHIGHWAVE', 'CDLHIKKAKE', 'CDLHIKKAKEMOD', 'CDLHOMINGPIGEON', 'CDLIDENTICAL3CROWS',
                'CDLINNECK', 'CDLINVERTEDHAMMER', 'CDLKICKING', 'CDLKICKINGBYLENGTH', 'CDLLADDERBOTTOM',
                'CDLLONGLEGGEDDOJI', 'CDLLONGLINE', 'CDLMARUBOZU', 'CDLMATCHINGLOW', 'CDLMATHOLD',
                'CDLMORNINGDOJISTAR', 'CDLMORNINGSTAR', 'CDLONNECK', 'CDLPIERCING', 'CDLRICKSHAWMAN',
                'CDLRISEFALL3METHODS', 'CDLSEPARATINGLINES', 'CDLSHOOTINGSTAR', 'CDLSHORTLINE',
                'CDLSPINNINGTOP', 'CDLSTALLEDPATTERN', 'CDLSTICKSANDWICH', 'CDLTAKURI', 'CDLTASUKIGAP',
                'CDLTHRUSTING', 'CDLTRISTAR', 'CDLUNIQUE3RIVER', 'CDLUPSIDEGAP2CROWS', 'CDLXSIDEGAP3METHODS'
            ]
            
            for pattern in candle_patterns:
                try:
                    patterns[pattern] = getattr(talib, pattern)(open_price, high, low, close)
                except Exception as e:
                    logger.warning(f"Failed to calculate {pattern}: {e}")
                    patterns[pattern] = np.zeros(len(data))
            
            patterns_df = pd.DataFrame(patterns, index=data.index)
            
            logger.info(f"è®¡ç®—äº† {len(patterns)} ä¸ªèœ¡çƒ›å›¾å½¢æ€æŒ‡æ ‡")
            return patterns_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—èœ¡çƒ›å›¾å½¢æ€å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_financial_indicators(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """è®¡ç®—è´¢åŠ¡æŒ‡æ ‡å’Œæ¢æ‰‹ç‡ï¼ˆçº¦15ä¸ªï¼‰- ä½¿ç”¨ä¼°ç®—å€¼æ›¿ä»£ç¼ºå¤±æ•°æ®"""
        try:
            result_data = data.copy()
            
            # é¢„å…ˆåˆå§‹åŒ–æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡åˆ—
            financial_columns = [
                'PriceToBookRatio', 'MarketCap', 'PERatio', 'PriceToSalesRatio',
                'ROE', 'ROA', 'ProfitMargins', 'CurrentRatio', 'QuickRatio', 
                'DebtToEquity', 'TobinsQ', 'DailyTurnover', 
                'turnover_c1d', 'turnover_c5d', 'turnover_c10d', 'turnover_c20d', 'turnover_c30d',
                'turnover_m5d', 'turnover_m10d', 'turnover_m20d', 'turnover_m30d'
            ]
            
            # è·å–åŸºæœ¬ä¿¡æ¯æ•°æ®
            info_data = self.get_financial_data(symbol, 'info')
            balance_sheet_data = self.get_financial_data(symbol, 'balance_sheet')
            
            # å¦‚æœæœ‰çœŸå®è´¢åŠ¡æ•°æ®ï¼Œä½¿ç”¨çœŸå®æ•°æ®
            if info_data is not None and not info_data.empty:
                result_data = self._calculate_real_financial_indicators(result_data, info_data, balance_sheet_data)
            else:
                # å¦åˆ™ä½¿ç”¨åŸºäºä»·æ ¼å’Œæˆäº¤é‡çš„ä¼°ç®—æŒ‡æ ‡
                result_data = self._calculate_estimated_financial_indicators(result_data, symbol)
            
            # ç¡®ä¿æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡åˆ—éƒ½å­˜åœ¨ä¸”æœ‰é»˜è®¤å€¼
            result_data = self._ensure_financial_columns_exist(result_data, symbol)
            
            logger.info(f"âœ… å®Œæˆè´¢åŠ¡æŒ‡æ ‡è®¡ç®— (åŒ…å«ä¼°ç®—å€¼)")
            return result_data
            
        except Exception as e:
            logger.error(f"è®¡ç®—è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿè¦ç¡®ä¿åˆ—å­˜åœ¨
            for col in financial_columns:
                if col not in data.columns:
                    data[col] = np.nan
            return data
    
    def _calculate_real_financial_indicators(self, data: pd.DataFrame, info_data: pd.DataFrame, balance_sheet_data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®è®¡ç®—æŒ‡æ ‡"""
        result_data = data.copy()
        
        try:
            # è·å–è´¢åŠ¡æ•°æ®çš„ç¬¬ä¸€è¡Œï¼ˆé€šå¸¸åŒ…å«æœ€æ–°æ•°æ®ï¼‰
            info_row = info_data.iloc[0] if not info_data.empty else pd.Series()
            
            # 1. å¸‚å‡€ç‡ (Price to Book Ratio)
            if 'priceToBook' in info_row.index and not pd.isna(info_row['priceToBook']):
                # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„å¸‚å‡€ç‡
                result_data['PriceToBookRatio'] = float(info_row['priceToBook'])
            elif 'bookValue' in info_row.index and not pd.isna(info_row['bookValue']):
                book_value = float(info_row['bookValue'])
                if book_value > 0:
                    result_data['PriceToBookRatio'] = result_data['Close'] / book_value
            
            # 2. å¸‚å€¼ (Market Cap)
            if 'marketCap' in info_row.index and not pd.isna(info_row['marketCap']):
                result_data['MarketCap'] = float(info_row['marketCap'])
            elif 'sharesOutstanding' in info_row.index and not pd.isna(info_row['sharesOutstanding']):
                shares_outstanding = float(info_row['sharesOutstanding'])
                if shares_outstanding > 0:
                    result_data['MarketCap'] = result_data['Close'] * shares_outstanding
            
            # 3. å¸‚ç›ˆç‡ (PE Ratio)
            if 'trailingPE' in info_row.index and not pd.isna(info_row['trailingPE']):
                result_data['PERatio'] = float(info_row['trailingPE'])
            elif 'forwardPE' in info_row.index and not pd.isna(info_row['forwardPE']):
                result_data['PERatio'] = float(info_row['forwardPE'])
            
            # 4. å¸‚é”€ç‡ (Price to Sales Ratio)
            if 'priceToSalesTrailing12Months' in info_row.index and not pd.isna(info_row['priceToSalesTrailing12Months']):
                result_data['PriceToSalesRatio'] = float(info_row['priceToSalesTrailing12Months'])
            
            # 5. å‡€èµ„äº§æ”¶ç›Šç‡ (ROE)
            if 'returnOnEquity' in info_row.index and not pd.isna(info_row['returnOnEquity']):
                result_data['ROE'] = float(info_row['returnOnEquity'])
            
            # 6. èµ„äº§æ”¶ç›Šç‡ (ROA)
            if 'returnOnAssets' in info_row.index and not pd.isna(info_row['returnOnAssets']):
                result_data['ROA'] = float(info_row['returnOnAssets'])
            
            # 7. åˆ©æ¶¦ç‡
            if 'profitMargins' in info_row.index and not pd.isna(info_row['profitMargins']):
                result_data['ProfitMargins'] = float(info_row['profitMargins'])
            
            # 8. æµåŠ¨æ¯”ç‡ (å¦‚æœæœ‰èµ„äº§è´Ÿå€ºè¡¨æ•°æ®)
            if balance_sheet_data is not None and not balance_sheet_data.empty:
                balance_row = balance_sheet_data.iloc[0]
                if 'currentRatio' in balance_row.index and not pd.isna(balance_row['currentRatio']):
                    result_data['CurrentRatio'] = float(balance_row['currentRatio'])
                elif 'Total Current Assets' in balance_row.index and 'Total Current Liabilities' in balance_row.index:
                    current_assets = balance_row.get('Total Current Assets', 0)
                    current_liabilities = balance_row.get('Total Current Liabilities', 1)
                    if current_liabilities > 0:
                        result_data['CurrentRatio'] = current_assets / current_liabilities
            
            # 9. é€ŸåŠ¨æ¯”ç‡
            if 'quickRatio' in info_row.index and not pd.isna(info_row['quickRatio']):
                result_data['QuickRatio'] = float(info_row['quickRatio'])
            else:
                # ä¼°ç®—ä¸ºæµåŠ¨æ¯”ç‡çš„80%
                if 'CurrentRatio' in result_data.columns:
                    result_data['QuickRatio'] = result_data['CurrentRatio'] * 0.8
            
            # 10. èµ„äº§è´Ÿå€ºç‡
            if 'debtToEquity' in info_row.index and not pd.isna(info_row['debtToEquity']):
                result_data['DebtToEquity'] = float(info_row['debtToEquity'])
            elif 'totalDebt' in info_row.index and 'marketCap' in info_row.index:
                total_debt = info_row.get('totalDebt', 0)
                market_cap = info_row.get('marketCap', 1)
                if market_cap > 0:
                    result_data['DebtToEquity'] = total_debt / market_cap
            
            # 11. æ‰˜å®¾Qå€¼
            if 'enterpriseValue' in info_row.index and balance_sheet_data is not None:
                enterprise_value = info_row.get('enterpriseValue', None)
                if enterprise_value and not balance_sheet_data.empty:
                    balance_row = balance_sheet_data.iloc[0]
                    total_assets = balance_row.get('Total Assets', balance_row.get('totalAssets', None))
                    if total_assets and total_assets > 0:
                        result_data['TobinsQ'] = enterprise_value / total_assets
            
            # 12. æ¢æ‰‹ç‡è®¡ç®—
            if 'floatShares' in info_row.index and not pd.isna(info_row['floatShares']):
                float_shares = float(info_row['floatShares'])
                if float_shares > 0:
                    result_data = self._calculate_real_turnover_indicators(result_data, float_shares)
            elif 'sharesOutstanding' in info_row.index and not pd.isna(info_row['sharesOutstanding']):
                # ä½¿ç”¨æ€»è‚¡æœ¬ä½œä¸ºæµé€šè‚¡çš„æ›¿ä»£
                shares_outstanding = float(info_row['sharesOutstanding'])
                if shares_outstanding > 0:
                    result_data = self._calculate_real_turnover_indicators(result_data, shares_outstanding)
            
            logger.info("âœ… ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®è®¡ç®—å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä½¿ç”¨çœŸå®è´¢åŠ¡æ•°æ®è®¡ç®—å¤±è´¥: {e}")
            # å¦‚æœçœŸå®æ•°æ®è®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—æ–¹æ³•
            result_data = self._calculate_estimated_financial_indicators(result_data, "UNKNOWN")
        
        return result_data
    
    def _calculate_real_turnover_indicators(self, data: pd.DataFrame, shares_count: float) -> pd.DataFrame:
        """åŸºäºçœŸå®æµé€šè‚¡æ•°è®¡ç®—æ¢æ‰‹ç‡æŒ‡æ ‡"""
        try:
            result_data = data.copy()
            
            # è®¡ç®—æ—¥æ¢æ‰‹ç‡
            result_data['DailyTurnover'] = result_data['Volume'] / shares_count
            
            # è®¡ç®—ä¸åŒçª—å£çš„ç´¯è®¡å’Œå¹³å‡æ¢æ‰‹ç‡
            windows = [1, 5, 10, 20, 30]
            for window in windows:
                result_data[f'turnover_c{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).sum()
                if window > 1:
                    result_data[f'turnover_m{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).mean()
            
            return result_data
            
        except Exception as e:
            logger.error(f"è®¡ç®—çœŸå®æ¢æ‰‹ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return data
    
    def _calculate_estimated_financial_indicators(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """åŸºäºä»·æ ¼å’Œæˆäº¤é‡æ•°æ®ä¼°ç®—è´¢åŠ¡æŒ‡æ ‡"""
        result_data = data.copy()
        
        # è·å–åŸºç¡€æ•°æ®
        close = result_data['Close'].values
        volume = result_data['Volume'].values
        high = result_data['High'].values
        low = result_data['Low'].values
        
        # 1. ä¼°ç®—å¸‚å€¼ (å‡è®¾æµé€šè‚¡ä¸ºå¹³å‡æˆäº¤é‡çš„æŸä¸ªå€æ•°)
        avg_volume = np.mean(volume[volume > 0]) if len(volume[volume > 0]) > 0 else 1000000
        estimated_shares = avg_volume * 50  # å‡è®¾å¹³å‡æˆäº¤é‡æ˜¯æµé€šè‚¡çš„1/50
        result_data['MarketCap'] = close * estimated_shares
        
        # 2. ä¼°ç®—å¸‚å‡€ç‡ (åŸºäºä»·æ ¼æ³¢åŠ¨æ€§ï¼Œé«˜æ³¢åŠ¨æ€§é€šå¸¸å¯¹åº”é«˜PB)
        price_volatility = pd.Series(close).rolling(20).std().fillna(0) / pd.Series(close).rolling(20).mean().fillna(1)
        result_data['PriceToBookRatio'] = 1.0 + price_volatility * 3  # åŸºå‡†1å€ï¼Œæ³¢åŠ¨æ€§æ¯å¢åŠ 1ï¼ŒPBå¢åŠ 3
        
        # 3. ä¼°ç®—å¸‚ç›ˆç‡ (åŸºäºä»·æ ¼è¶‹åŠ¿ï¼Œä¸Šæ¶¨è¶‹åŠ¿å¯¹åº”é«˜PE)
        price_trend = pd.Series(close).rolling(20).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0).fillna(0)
        base_pe = 15  # åŸºå‡†PE
        result_data['PERatio'] = base_pe + (price_trend / np.mean(close) * 1000)
        result_data['PERatio'] = np.clip(result_data['PERatio'], 5, 50)  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        
        # 4. ä¼°ç®—å¸‚é”€ç‡ (åŸºäºæˆäº¤é‡æ´»è·ƒåº¦)
        volume_series = pd.Series(volume, index=result_data.index)
        volume_activity = volume_series.rolling(20).mean().fillna(avg_volume) / avg_volume
        result_data['PriceToSalesRatio'] = 1.0 + volume_activity * 2  # æˆäº¤æ´»è·ƒå¯¹åº”é«˜PS
        
        # 5. ä¼°ç®—ROE (åŸºäºæ”¶ç›Šç‡)
        returns = pd.Series(close).pct_change(20).fillna(0)
        result_data['ROE'] = np.clip(returns * 4, -0.3, 0.5)  # å¹´åŒ–æ”¶ç›Šç‡ä½œä¸ºROEçš„ä»£ç†
        
        # 6. ä¼°ç®—ROA (é€šå¸¸æ¯”ROEä½)
        result_data['ROA'] = result_data['ROE'] * 0.6
        
        # 7. ä¼°ç®—åˆ©æ¶¦ç‡ (åŸºäºä»·æ ¼ç¨³å®šæ€§)
        price_stability = 1 / (1 + price_volatility)
        result_data['ProfitMargins'] = price_stability * 0.1  # ç¨³å®šçš„è‚¡ç¥¨å‡è®¾æœ‰æ›´å¥½çš„åˆ©æ¶¦ç‡
        
        # 8. ä¼°ç®—æµåŠ¨æ¯”ç‡ (åŸºäºæˆäº¤é‡æµåŠ¨æ€§)
        volume_series = pd.Series(volume, index=result_data.index)
        liquidity = volume_series.rolling(5).mean() / volume_series.rolling(20).mean()
        result_data['CurrentRatio'] = 1.0 + liquidity.fillna(1) * 0.5
        
        # 9. ä¼°ç®—é€ŸåŠ¨æ¯”ç‡ (é€šå¸¸æ¯”æµåŠ¨æ¯”ç‡ä½)
        result_data['QuickRatio'] = result_data['CurrentRatio'] * 0.8
        
        # 10. ä¼°ç®—èµ„äº§è´Ÿå€ºç‡ (åŸºäºæ³¢åŠ¨æ€§ï¼Œé«˜æ³¢åŠ¨å¯èƒ½æ„å‘³ç€é«˜æ æ†)
        result_data['DebtToEquity'] = price_volatility * 2
        
        # 11. ä¼°ç®—æ‰˜å®¾Qå€¼
        market_value_ratio = (high + low) / (2 * close)  # ç›¸å¯¹ä¼°å€¼
        market_value_ratio = pd.Series(market_value_ratio, index=result_data.index).fillna(1)
        result_data['TobinsQ'] = market_value_ratio
        
        # 12. ä¼°ç®—æ¢æ‰‹ç‡æŒ‡æ ‡
        result_data = self._calculate_estimated_turnover_indicators(result_data, estimated_shares)
        
        logger.info(f"ğŸ”® ä½¿ç”¨ä¼°ç®—æ–¹æ³•è®¡ç®—è´¢åŠ¡æŒ‡æ ‡ (åŸºäºä»·æ ¼å’Œæˆäº¤é‡)")
        return result_data
    
    def _calculate_estimated_turnover_indicators(self, data: pd.DataFrame, estimated_shares: float) -> pd.DataFrame:
        """åŸºäºä¼°ç®—æµé€šè‚¡è®¡ç®—æ¢æ‰‹ç‡æŒ‡æ ‡"""
        try:
            result_data = data.copy()
            
            # è®¡ç®—æ—¥æ¢æ‰‹ç‡
            result_data['DailyTurnover'] = result_data['Volume'] / estimated_shares
            
            # è®¡ç®—ä¸åŒçª—å£çš„ç´¯è®¡å’Œå¹³å‡æ¢æ‰‹ç‡
            windows = [1, 5, 10, 20, 30]
            for window in windows:
                result_data[f'turnover_c{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).sum()
                if window > 1:
                    result_data[f'turnover_m{window}d'] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).mean()
            
            logger.info("âœ… å®Œæˆæ¢æ‰‹ç‡æŒ‡æ ‡ä¼°ç®—")
            return result_data
            
        except Exception as e:
            logger.error(f"ä¼°ç®—æ¢æ‰‹ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return data
    
    def _ensure_financial_columns_exist(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """ç¡®ä¿æ‰€æœ‰è´¢åŠ¡æŒ‡æ ‡åˆ—éƒ½å­˜åœ¨ä¸”æœ‰åˆç†çš„é»˜è®¤å€¼"""
        result_data = data.copy()
        
        # å®šä¹‰æ‰€æœ‰å¿…éœ€çš„è´¢åŠ¡æŒ‡æ ‡åˆ—å’Œå…¶é»˜è®¤å€¼
        required_columns = {
            'PriceToBookRatio': 1.5,    # é»˜è®¤å¸‚å‡€ç‡
            'MarketCap': None,          # å¸‚å€¼éœ€è¦è®¡ç®—
            'PERatio': 15.0,            # é»˜è®¤å¸‚ç›ˆç‡
            'PriceToSalesRatio': 2.0,   # é»˜è®¤å¸‚é”€ç‡
            'ROE': 0.1,                 # é»˜è®¤10%çš„ROE
            'ROA': 0.05,                # é»˜è®¤5%çš„ROA
            'ProfitMargins': 0.08,      # é»˜è®¤8%çš„åˆ©æ¶¦ç‡
            'CurrentRatio': 1.2,        # é»˜è®¤æµåŠ¨æ¯”ç‡
            'QuickRatio': 1.0,          # é»˜è®¤é€ŸåŠ¨æ¯”ç‡
            'DebtToEquity': 0.5,        # é»˜è®¤èµ„äº§è´Ÿå€ºç‡
            'TobinsQ': 1.0,             # é»˜è®¤æ‰˜å®¾Qå€¼
            'DailyTurnover': None,      # éœ€è¦è®¡ç®—
            'turnover_c1d': None,       # éœ€è¦è®¡ç®—
            'turnover_c5d': None,       # éœ€è¦è®¡ç®—
            'turnover_c10d': None,      # éœ€è¦è®¡ç®—
            'turnover_c20d': None,      # éœ€è¦è®¡ç®—
            'turnover_c30d': None,      # éœ€è¦è®¡ç®—
            'turnover_m5d': None,       # éœ€è¦è®¡ç®—
            'turnover_m10d': None,      # éœ€è¦è®¡ç®—
            'turnover_m20d': None,      # éœ€è¦è®¡ç®—
            'turnover_m30d': None       # éœ€è¦è®¡ç®—
        }
        
        # ä¸ºç¼ºå¤±çš„åˆ—æ·»åŠ é»˜è®¤å€¼
        for col_name, default_value in required_columns.items():
            # æ£€æŸ¥åˆ—æ˜¯å¦ä¸å­˜åœ¨æˆ–å…¨éƒ¨ä¸ºNaN
            col_missing = col_name not in result_data.columns 
            col_all_nan = not col_missing and result_data[col_name].isna().all()
            col_all_zero = not col_missing and (result_data[col_name] == 0).all()
            
            # å¯¹äºæ¢æ‰‹ç‡æŒ‡æ ‡ï¼Œé¢å¤–æ£€æŸ¥æ˜¯å¦å…¨éƒ¨ä¸º0ï¼ˆè¿™é€šå¸¸è¡¨ç¤ºè®¡ç®—æœ‰é—®é¢˜ï¼‰
            needs_calculation = col_missing or col_all_nan or (col_name.startswith('turnover') and col_all_zero)
            
            if needs_calculation:
                if default_value is not None:
                    result_data[col_name] = default_value
                elif col_name == 'MarketCap':
                    # ä¼°ç®—å¸‚å€¼ï¼šå‡è®¾å¹³å‡è‚¡ä»·ä¸ºå½“å‰è‚¡ä»·ï¼Œæµé€šè‚¡ä¸ºæˆäº¤é‡çš„50å€
                    avg_volume = result_data['Volume'].mean() if 'Volume' in result_data.columns else 1000000
                    estimated_shares = avg_volume * 50
                    result_data['MarketCap'] = result_data['Close'] * estimated_shares
                elif col_name.startswith('turnover'):
                    # åªæœ‰åœ¨æ¢æ‰‹ç‡æŒ‡æ ‡ç¡®å®ç¼ºå¤±æˆ–æœ‰é—®é¢˜æ—¶æ‰é‡æ–°è®¡ç®—
                    logger.warning(f"æ¢æ‰‹ç‡æŒ‡æ ‡ {col_name} ç¼ºå¤±æˆ–å¼‚å¸¸ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
                    
                    # ä¼°ç®—æ¢æ‰‹ç‡ç›¸å…³æŒ‡æ ‡
                    if 'DailyTurnover' not in result_data.columns or result_data['DailyTurnover'].isna().all() or (result_data['DailyTurnover'] == 0).all():
                        avg_volume = result_data['Volume'].mean() if 'Volume' in result_data.columns else 1000000
                        estimated_shares = avg_volume * 50
                        result_data['DailyTurnover'] = result_data['Volume'] / estimated_shares
                    
                    # è®¡ç®—ç´¯è®¡å’Œå¹³å‡æ¢æ‰‹ç‡
                    if col_name.startswith('turnover_c'):
                        window = int(col_name.split('d')[0].split('_c')[1])
                        result_data[col_name] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).sum()
                    elif col_name.startswith('turnover_m'):
                        window = int(col_name.split('d')[0].split('_m')[1])
                        result_data[col_name] = result_data['DailyTurnover'].rolling(window=window, min_periods=1).mean()
        
        return result_data
    

    
    def calculate_volatility_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡ï¼ˆçº¦8ä¸ªï¼‰"""
        try:
            volatility_data = {}
            
            # è®¡ç®—ä»·æ ¼å˜åŒ–
            price_diff = data['Close'].diff()
            log_returns = np.log(data['Close'] / data['Close'].shift(1))
            
            # 1. å·²å®ç°æ³¢åŠ¨ç‡ (20å¤©çª—å£)
            volatility_data['RealizedVolatility_20'] = price_diff.rolling(window=20).std() * np.sqrt(252)
            
            # 2. å·²å®ç°è´ŸåŠå˜å·®
            negative_returns = price_diff[price_diff < 0]
            volatility_data['NegativeSemiDeviation_20'] = negative_returns.rolling(window=20).std() * np.sqrt(252)
            
            # 3. å·²å®ç°è¿ç»­æ³¢åŠ¨ç‡
            volatility_data['ContinuousVolatility_20'] = log_returns.rolling(window=20).std() * np.sqrt(252)
            
            # 4. å·²å®ç°æ­£åŠå˜å·®
            positive_returns = price_diff[price_diff > 0]
            volatility_data['PositiveSemiDeviation_20'] = positive_returns.rolling(window=20).std() * np.sqrt(252)
            
            # 5. ä¸åŒçª—å£çš„æ³¢åŠ¨ç‡
            for window in [10, 30, 60]:
                volatility_data[f'Volatility_{window}'] = price_diff.rolling(window=window).std() * np.sqrt(252)
            
            volatility_df = pd.DataFrame(volatility_data, index=data.index)
            
            logger.info("è®¡ç®—äº†æ³¢åŠ¨ç‡æŒ‡æ ‡")
            return volatility_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_all_indicators_for_stock(self, symbol: str) -> Optional[pd.DataFrame]:
        """ä¸ºå•åªè‚¡ç¥¨è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆæ”¯æŒå¹¶è¡Œè®¡ç®—ï¼‰"""
        try:
            # è¯»å–å†å²ä»·æ ¼æ•°æ®
            price_data = self.read_qlib_binary_data(symbol)
            if price_data is None or price_data.empty:
                logger.warning(f"No price data found for {symbol}")
                return None
            
            # ä½¿ç”¨å¹¶è¡Œè®¡ç®—æˆ–é¡ºåºè®¡ç®—
            if self.enable_parallel:
                return self._calculate_indicators_parallel(symbol, price_data)
            else:
                return self._calculate_indicators_sequential(symbol, price_data)
                
        except Exception as e:
            logger.error(f"âŒ {symbol}: è®¡ç®—æŒ‡æ ‡å¤±è´¥ - {e}")
            return None
    
    def _calculate_indicators_parallel(self, symbol: str, price_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        å¹¶è¡Œè®¡ç®—å•åªè‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡ç±»å‹
        """
        if not self.enable_parallel:
            return self._calculate_indicators_sequential(symbol, price_data)
        
        try:
            logger.info(f"å¼€å§‹å¹¶è¡Œè®¡ç®— {symbol} çš„æ‰€æœ‰æŒ‡æ ‡...")
            start_time = time.time()
            
            # é‡ç½®æŒ‡æ ‡ç¼“å­˜
            self._reset_indicators_cache()
            
            # ä¿å­˜åŸå§‹æ—¥æœŸä¿¡æ¯
            original_dates = price_data.index
            
            # å®šä¹‰å„ç±»æŒ‡æ ‡è®¡ç®—ä»»åŠ¡
            indicator_tasks = [
                ('Alpha158', partial(self.calculate_alpha158_indicators, price_data)),
                ('Alpha360', partial(self.calculate_alpha360_indicators, price_data)),
                ('Technical', partial(self.calculate_all_technical_indicators, price_data)),
                ('Candlestick', partial(self.calculate_candlestick_patterns, price_data)),
                ('Financial', partial(self.calculate_financial_indicators, price_data, symbol)),
                ('Volatility', partial(self.calculate_volatility_indicators, price_data))
            ]
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè®¡ç®—
            results = {}
            failed_tasks = []
            
            with ThreadPoolExecutor(max_workers=min(6, self.max_workers)) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_task = {
                    executor.submit(task_func): task_name 
                    for task_name, task_func in indicator_tasks
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task_name = future_to_task[future]
                    try:
                        result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                        if result is not None and not result.empty:
                            results[task_name] = result
                            logger.debug(f"âœ… {symbol} - {task_name}: {result.shape[1]} ä¸ªæŒ‡æ ‡")
                        else:
                            failed_tasks.append(task_name)
                            logger.warning(f"âš ï¸ {symbol} - {task_name}: è®¡ç®—ç»“æœä¸ºç©º")
                    except Exception as e:
                        failed_tasks.append(task_name)
                        logger.error(f"âŒ {symbol} - {task_name}: è®¡ç®—å¤±è´¥ - {e}")
            
            if failed_tasks:
                logger.warning(f"{symbol}: ä»¥ä¸‹æŒ‡æ ‡ç±»å‹è®¡ç®—å¤±è´¥: {failed_tasks}")
            
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
            try:
                all_indicators = [price_data]
                all_indicators.extend(results.values())
                
                if all_indicators:
                    # ç¡®ä¿æ‰€æœ‰DataFrameéƒ½æœ‰ä¸€è‡´çš„ç´¢å¼•ï¼Œä½†ä¿ç•™æ—¥æœŸä¿¡æ¯
                    aligned_indicators = []
                    base_length = len(price_data)
                    
                    for df in all_indicators:
                        if df is not None and not df.empty:
                            # é‡ç½®ç´¢å¼•ä½†ä¿ç•™åŸå§‹ç´¢å¼•ä½œä¸ºDateåˆ—ï¼ˆå¦‚æœè¿˜æ²¡æœ‰Dateåˆ—çš„è¯ï¼‰
                            df_reset = df.reset_index()
                            if 'index' in df_reset.columns and 'Date' not in df_reset.columns:
                                df_reset = df_reset.rename(columns={'index': 'Date'})
                            elif 'Date' not in df_reset.columns:
                                # å¦‚æœæ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸ
                                df_reset['Date'] = original_dates[:len(df_reset)]
                            
                            # ç¡®ä¿é•¿åº¦ä¸€è‡´
                            if len(df_reset) != base_length:
                                # é‡æ–°é‡‡æ ·æˆ–æˆªæ–­ä»¥åŒ¹é…åŸºå‡†é•¿åº¦
                                if len(df_reset) > base_length:
                                    df_reset = df_reset.iloc[:base_length]
                                else:
                                    # ç”¨NaNå¡«å……ä¸è¶³çš„è¡Œ
                                    missing_rows = base_length - len(df_reset)
                                    padding_data = {}
                                    for col in df_reset.columns:
                                        if col == 'Date':
                                            # ä¸ºæ—¥æœŸåˆ—ç”Ÿæˆåˆé€‚çš„æ—¥æœŸ
                                            last_date = df_reset['Date'].iloc[-1] if len(df_reset) > 0 else original_dates[-1]
                                            if isinstance(last_date, str):
                                                last_date = pd.to_datetime(last_date)
                                            additional_dates = pd.bdate_range(start=last_date + pd.Timedelta(days=1), periods=missing_rows)
                                            padding_data[col] = additional_dates
                                        else:
                                            padding_data[col] = [np.nan] * missing_rows
                                    
                                    padding_df = pd.DataFrame(padding_data)
                                    df_reset = pd.concat([df_reset, padding_df], ignore_index=True)
                            aligned_indicators.append(df_reset)
                    
                    # å®‰å…¨åˆå¹¶ - ä¿ç•™Dateåˆ—
                    combined_df = pd.concat(aligned_indicators, axis=1)
                    
                    # å¤„ç†é‡å¤çš„Dateåˆ—
                    if 'Date' in combined_df.columns:
                        date_cols = [col for col in combined_df.columns if col == 'Date']
                        if len(date_cols) > 1:
                            # ä¿ç•™ç¬¬ä¸€ä¸ªDateåˆ—ï¼Œåˆ é™¤å…¶ä½™çš„
                            combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]
                    
                    # ä»è´¢åŠ¡æ•°æ®ä¸­æå–æ–°å¢çš„åˆ—
                    if 'Financial' in results:
                        financial_data = results['Financial']
                        financial_cols = [col for col in financial_data.columns if col not in price_data.columns and col != 'Date']
                        if financial_cols:
                            for col in financial_cols:
                                if col not in combined_df.columns:
                                    # é‡æ–°ç´¢å¼•è´¢åŠ¡æ•°æ®ä»¥åŒ¹é…åŸºå‡†ç´¢å¼•
                                    financial_col_data = financial_data[col].reindex(range(base_length), method='ffill')
                                    combined_df[col] = financial_col_data
                    
                    # æ·»åŠ è‚¡ç¥¨ä»£ç 
                    combined_df['Symbol'] = symbol
                    
                    # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šDate, Symbol, ç„¶åæ˜¯å…¶ä»–åˆ—
                    cols = []
                    if 'Date' in combined_df.columns:
                        cols.append('Date')
                    cols.append('Symbol')
                    cols.extend([col for col in combined_df.columns if col not in ['Date', 'Symbol']])
                    combined_df = combined_df[cols]
                    
                    # é‡ç½®æ•°å­—ç´¢å¼•ï¼Œä½†ä¿ç•™Dateåˆ—
                    combined_df = combined_df.reset_index(drop=True)
                    
                    elapsed_time = time.time() - start_time
                    logger.info(f"âœ… {symbol}: å¹¶è¡Œè®¡ç®—å®Œæˆ {len(combined_df.columns)-2} ä¸ªæŒ‡æ ‡ (è€—æ—¶: {elapsed_time:.2f}s)")
                    return combined_df
                    
            except Exception as e:
                logger.error(f"âŒ {symbol}: åˆå¹¶æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯ - {e}")
                # é™çº§åˆ°é¡ºåºè®¡ç®—æ–¹æ³•
                return self._calculate_indicators_sequential(symbol, price_data)
            else:
                logger.error(f"âŒ {symbol}: æ‰€æœ‰æŒ‡æ ‡è®¡ç®—éƒ½å¤±è´¥äº†")
                return None
                
        except Exception as e:
            logger.error(f"âŒ {symbol}: å¹¶è¡Œè®¡ç®—å¤±è´¥ - {e}")
            return self._calculate_indicators_sequential(symbol, price_data)
    
    def _calculate_indicators_sequential(self, symbol: str, price_data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """
        é¡ºåºè®¡ç®—å•åªè‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
        """
        try:
            logger.info(f"å¼€å§‹é¡ºåºè®¡ç®— {symbol} çš„æ‰€æœ‰æŒ‡æ ‡...")
            
            # é‡ç½®æŒ‡æ ‡è·Ÿè¸ªå™¨
            self._reset_indicators_cache()
            
            # ä¿å­˜åŸå§‹æ—¥æœŸä¿¡æ¯
            original_dates = price_data.index
            
            # 1. è®¡ç®—Alpha158æŒ‡æ ‡ä½“ç³» (~158ä¸ª)
            alpha158_indicators = self.calculate_alpha158_indicators(price_data)
            
            # 2. è®¡ç®—Alpha360æŒ‡æ ‡ä½“ç³» (~360ä¸ª)
            alpha360_indicators = self.calculate_alpha360_indicators(price_data)
            
            # 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (~60ä¸ª)
            technical_indicators = self.calculate_all_technical_indicators(price_data)
            
            # 4. è®¡ç®—èœ¡çƒ›å›¾å½¢æ€ (61ä¸ª)
            candlestick_patterns = self.calculate_candlestick_patterns(price_data)
            
            # 5. è®¡ç®—è´¢åŠ¡æŒ‡æ ‡ (~15ä¸ª)
            financial_data = self.calculate_financial_indicators(price_data, symbol)
            
            # 6. è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡ (~8ä¸ª)
            volatility_indicators = self.calculate_volatility_indicators(price_data)
            
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡ï¼ˆç¡®ä¿ç´¢å¼•ä¸€è‡´æ€§å¹¶ä¿ç•™æ—¥æœŸä¿¡æ¯ï¼‰
            base_index = price_data.index
            indicator_dfs = [
                price_data,
                alpha158_indicators,
                alpha360_indicators,
                technical_indicators,
                candlestick_patterns,
                financial_data,  # æ·»åŠ è´¢åŠ¡æ•°æ®
                volatility_indicators
            ]
            
            # é‡æ–°ç´¢å¼•æ‰€æœ‰DataFrameä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼Œå¹¶ä¿ç•™æ—¥æœŸä¿¡æ¯
            aligned_dfs = []
            for df in indicator_dfs:
                if df is not None and not df.empty:
                    if not df.index.equals(base_index):
                        df = df.reindex(base_index, method='ffill')
                    
                    # å°†ç´¢å¼•è½¬æ¢ä¸ºDateåˆ—ï¼ˆå¦‚æœè¿˜ä¸æ˜¯åˆ—çš„è¯ï¼‰
                    df_with_date = df.reset_index()
                    if 'index' in df_with_date.columns and 'Date' not in df_with_date.columns:
                        df_with_date = df_with_date.rename(columns={'index': 'Date'})
                    elif 'Date' not in df_with_date.columns:
                        df_with_date['Date'] = original_dates
                    
                    aligned_dfs.append(df_with_date)
            
            all_indicators = pd.concat(aligned_dfs, axis=1)
            
            # å¤„ç†é‡å¤çš„Dateåˆ—
            if 'Date' in all_indicators.columns:
                date_cols = [col for col in all_indicators.columns if col == 'Date']
                if len(date_cols) > 1:
                    # ä¿ç•™ç¬¬ä¸€ä¸ªDateåˆ—ï¼Œåˆ é™¤å…¶ä½™çš„
                    all_indicators = all_indicators.loc[:, ~all_indicators.columns.duplicated(keep='first')]
            
            # è´¢åŠ¡æ•°æ®å·²ç»åœ¨ä¸»åˆå¹¶ä¸­å¤„ç†ï¼Œæ— éœ€é¢å¤–æ“ä½œ
            
            # æ·»åŠ è‚¡ç¥¨ä»£ç 
            all_indicators['Symbol'] = symbol
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåºï¼šDate, Symbol, ç„¶åæ˜¯å…¶ä»–åˆ—
            cols = []
            if 'Date' in all_indicators.columns:
                cols.append('Date')
            cols.append('Symbol')
            cols.extend([col for col in all_indicators.columns if col not in ['Date', 'Symbol']])
            all_indicators = all_indicators[cols]
            
            # é‡ç½®æ•°å­—ç´¢å¼•ï¼Œä½†ä¿ç•™Dateåˆ—
            all_indicators = all_indicators.reset_index(drop=True)
            
            logger.info(f"âœ… {symbol}: é¡ºåºè®¡ç®—å®Œæˆ {len(all_indicators.columns)-2} ä¸ªæŒ‡æ ‡")
            return all_indicators
            
        except Exception as e:
            logger.error(f"âŒ {symbol}: é¡ºåºè®¡ç®—å¤±è´¥ - {e}")
            return None
    
    def calculate_all_indicators(self, max_stocks: Optional[int] = None) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        stocks = self.get_available_stocks()
        
        if max_stocks:
            stocks = stocks[:max_stocks]
        
        if not stocks:
            logger.error("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è‚¡ç¥¨æ•°æ®")
            return pd.DataFrame()
        
        logger.info(f"å¼€å§‹è®¡ç®— {len(stocks)} åªè‚¡ç¥¨çš„æŒ‡æ ‡...")
        start_time = time.time()
        
        if self.enable_parallel and len(stocks) > 1:
            return self._calculate_all_stocks_parallel(stocks)
        else:
            return self._calculate_all_stocks_sequential(stocks)
    
    def _calculate_all_stocks_parallel(self, stocks: List[str]) -> pd.DataFrame:
        """å¹¶è¡Œè®¡ç®—å¤šåªè‚¡ç¥¨çš„æŒ‡æ ‡"""
        logger.info(f"ä½¿ç”¨å¹¶è¡Œæ¨¡å¼è®¡ç®— {len(stocks)} åªè‚¡ç¥¨ (æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers})")
        
        all_results = []
        success_count = 0
        failed_stocks = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰è‚¡ç¥¨çš„è®¡ç®—ä»»åŠ¡
            future_to_symbol = {
                executor.submit(self.calculate_all_indicators_for_stock, symbol): symbol 
                for symbol in stocks
            }
            
            # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
            completed = 0
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                completed += 1
                
                try:
                    result = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                    if result is not None:
                        all_results.append(result)
                        success_count += 1
                        logger.info(f"âœ… è¿›åº¦ {completed}/{len(stocks)}: {symbol} è®¡ç®—å®Œæˆ ({len(result.columns)-1} ä¸ªæŒ‡æ ‡)")
                    else:
                        failed_stocks.append(symbol)
                        logger.warning(f"âš ï¸ è¿›åº¦ {completed}/{len(stocks)}: {symbol} è®¡ç®—ç»“æœä¸ºç©º")
                        
                except Exception as e:
                    failed_stocks.append(symbol)
                    logger.error(f"âŒ è¿›åº¦ {completed}/{len(stocks)}: {symbol} è®¡ç®—å¤±è´¥ - {e}")
        
        elapsed_time = time.time() - start_time
        
        if failed_stocks:
            logger.warning(f"è®¡ç®—å¤±è´¥çš„è‚¡ç¥¨ ({len(failed_stocks)}): {failed_stocks[:5]}{'...' if len(failed_stocks) > 5 else ''}")
        
        if all_results:
            try:
                # æ•°æ®åˆå¹¶å‰çš„é¢„å¤„ç†å’Œæ£€æŸ¥
                logger.info("å¼€å§‹åˆå¹¶å¤šåªè‚¡ç¥¨çš„è®¡ç®—ç»“æœ...")
                
                # å¼ºåŒ–çš„DataFrameæ¸…ç†å’Œæ ‡å‡†åŒ–
                logger.info("å¼€å§‹å¼ºåŒ–çš„DataFrameæ¸…ç†å’Œæ ‡å‡†åŒ–...")
                
                # ç¬¬ä¸€æ­¥ï¼šåˆæ­¥æ¸…ç†å’ŒéªŒè¯
                valid_dfs = []
                for i, df in enumerate(all_results):
                    if df is None or df.empty:
                        logger.warning(f"è·³è¿‡ç©ºçš„DataFrame (ç´¢å¼•: {i})")
                        continue
                    
                    # å®Œå…¨é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ä¸ºè¿ç»­æ•°å­—ç´¢å¼•
                    df_clean = df.copy()
                    df_clean = df_clean.reset_index(drop=True)
                    
                    # æ£€æŸ¥ç´¢å¼•å”¯ä¸€æ€§
                    if df_clean.index.has_duplicates:
                        logger.warning(f"DataFrame {i} å­˜åœ¨é‡å¤ç´¢å¼•ï¼Œè¿›è¡Œå»é‡")
                        df_clean = df_clean.loc[~df_clean.index.duplicated(keep='first')]
                        df_clean = df_clean.reset_index(drop=True)
                    
                    valid_dfs.append((i, df_clean))
                
                if not valid_dfs:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœå¯ä»¥å¤„ç†")
                    return pd.DataFrame()
                
                # ç¬¬äºŒæ­¥ï¼šç»Ÿä¸€åˆ—ç»“æ„
                logger.info(f"ç»Ÿä¸€ {len(valid_dfs)} ä¸ªDataFrameçš„åˆ—ç»“æ„...")
                
                # è·å–æ‰€æœ‰å”¯ä¸€çš„åˆ—å
                all_columns = set()
                for _, df in valid_dfs:
                    all_columns.update(df.columns)
                
                all_columns = sorted(list(all_columns))
                logger.info(f"å‘ç° {len(all_columns)} ä¸ªå”¯ä¸€åˆ—")
                
                # ä½¿ç”¨ç®€å•å®‰å…¨çš„åˆå¹¶æ–¹æ³•
                cleaned_results = []
                for i, df in valid_dfs:
                    try:
                        # ç®€å•å¤åˆ¶å’Œé‡ç½®ç´¢å¼•
                        clean_df = df.copy()
                        clean_df = clean_df.reset_index(drop=True)
                        
                        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹æˆ–å­—ç¬¦ä¸²ï¼Œä½†ä¿æŠ¤é‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—
                        # éœ€è¦ä¿æŠ¤çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—
                        protected_cols = ['Symbol', 'Ticker', 'Name', 'ENName', 'Code', 'Date']
                        
                        for col in clean_df.columns:
                            try:
                                # è·³è¿‡é‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—ï¼Œä¸è¿›è¡Œæ•°å€¼è½¬æ¢
                                if col in protected_cols:
                                    continue
                                    
                                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¦‚æœå¤±è´¥å°±ä¿æŒåŸæ ·
                                if clean_df[col].dtype == 'object':
                                    try:
                                        clean_df[col] = pd.to_numeric(clean_df[col], errors='coerce')
                                    except:
                                        pass
                            except:
                                pass
                        
                        cleaned_results.append(clean_df)
                        logger.debug(f"âœ… æˆåŠŸæ¸…ç†DataFrame {i}")
                        
                    except Exception as e:
                        logger.error(f"âŒ æ¸…ç†DataFrame {i} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        continue
                
                if not cleaned_results:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœå¯ä»¥åˆå¹¶")
                    return pd.DataFrame()
                
                # å®‰å…¨åˆå¹¶DataFrame - ä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•
                logger.info(f"æ­£åœ¨åˆå¹¶ {len(cleaned_results)} ä¸ªæœ‰æ•ˆç»“æœ...")
                try:
                    # é€ä¸ªåˆå¹¶ï¼Œé¿å…åˆ—ä¸åŒ¹é…çš„é—®é¢˜
                    combined_df = None
                    for i, df in enumerate(cleaned_results):
                        if combined_df is None:
                            combined_df = df.copy()
                        else:
                            # ä½¿ç”¨outer joinç¡®ä¿æ‰€æœ‰åˆ—éƒ½è¢«ä¿ç•™
                            combined_df = pd.concat([combined_df, df], ignore_index=True, sort=False)
                        logger.debug(f"åˆå¹¶ç¬¬ {i+1}/{len(cleaned_results)} ä¸ªDataFrame")
                    
                    if combined_df is None or combined_df.empty:
                        logger.error("âŒ åˆå¹¶åçš„DataFrameä¸ºç©º")
                        return pd.DataFrame()
                    
                except Exception as merge_error:
                    logger.error(f"âŒ DataFrameåˆå¹¶å¤±è´¥: {merge_error}")
                    # é™çº§åˆ°æœ€ç®€å•çš„æ–¹æ³•
                    logger.info("å°è¯•ä½¿ç”¨æœ€ç®€å•çš„åˆå¹¶æ–¹æ³•...")
                    try:
                        # åªä¿ç•™åˆ—æ•°æœ€å°‘çš„DataFrameçš„åˆ—
                        min_cols = min(len(df.columns) for df in cleaned_results)
                        logger.info(f"ä½¿ç”¨æœ€å°åˆ—æ•°: {min_cols}")
                        
                        # æ‰¾åˆ°æœ‰æœ€å°åˆ—æ•°çš„ç¬¬ä¸€ä¸ªDataFrameä½œä¸ºæ¨¡æ¿
                        template_df = next(df for df in cleaned_results if len(df.columns) == min_cols)
                        template_cols = template_df.columns.tolist()
                        
                        # åªä¿ç•™å…¬å…±åˆ—è¿›è¡Œåˆå¹¶
                        aligned_dfs = []
                        for df in cleaned_results:
                            aligned_df = df[template_cols].copy()
                            aligned_dfs.append(aligned_df)
                        
                        combined_df = pd.concat(aligned_dfs, ignore_index=True)
                        logger.info(f"âœ… ç®€åŒ–åˆå¹¶æˆåŠŸï¼Œä¿ç•™ {len(template_cols)} åˆ—")
                        
                    except Exception as e2:
                        logger.error(f"âŒ ç®€åŒ–åˆå¹¶ä¹Ÿå¤±è´¥: {e2}")
                        return pd.DataFrame()
                
                # éªŒè¯åˆå¹¶ç»“æœ
                if combined_df.empty:
                    logger.error("âŒ åˆå¹¶åçš„DataFrameä¸ºç©º")
                    return pd.DataFrame()
                
                # æ£€æŸ¥é‡å¤è¡Œ
                initial_rows = len(combined_df)
                combined_df = combined_df.drop_duplicates()
                if len(combined_df) < initial_rows:
                    logger.info(f"ç§»é™¤äº† {initial_rows - len(combined_df)} è¡Œé‡å¤æ•°æ®")
                
                logger.info(f"âœ… å¹¶è¡Œè®¡ç®—å®Œæˆ: {success_count}/{len(stocks)} åªè‚¡ç¥¨æˆåŠŸ (è€—æ—¶: {elapsed_time:.2f}s)")
                # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                indicator_count = len(combined_df.columns) - (2 if 'Date' in combined_df.columns else 1)
                logger.info(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°é‡: {indicator_count}")
                logger.info(f"ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
                logger.info(f"âš¡ å¹³å‡æ¯åªè‚¡ç¥¨è€—æ—¶: {elapsed_time/len(stocks):.2f}s")
                return combined_df
                
            except Exception as e:
                logger.error(f"âŒ åˆå¹¶è®¡ç®—ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                logger.error(f"å°è¯•é™çº§ä¸ºç®€å•åˆå¹¶...")
                
                # é™çº§æ–¹æ¡ˆï¼šæœ€å®‰å…¨çš„é€ä¸ªåˆå¹¶
                try:
                    logger.info("ä½¿ç”¨æœ€å®‰å…¨çš„é€ä¸ªåˆå¹¶æ–¹æ¡ˆ...")
                    
                    # åªä¿ç•™éç©ºçš„ç»“æœ
                    valid_results = [df for df in all_results if df is not None and not df.empty]
                    
                    if not valid_results:
                        logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœ")
                        return pd.DataFrame()
                    
                    logger.info(f"å¼€å§‹é€ä¸ªåˆå¹¶ {len(valid_results)} ä¸ªDataFrame...")
                    
                    # é€ä¸ªå®‰å…¨åˆå¹¶
                    combined_df = None
                    successful_merges = 0
                    
                    for i, df in enumerate(valid_results):
                        try:
                            # å½»åº•æ¸…ç†å•ä¸ªDataFrame
                            clean_df = df.copy()
                            clean_df = clean_df.reset_index(drop=True)
                            
                            # ç¡®ä¿åˆ—åä¸ºå­—ç¬¦ä¸²
                            clean_df.columns = [str(col) for col in clean_df.columns]
                            
                            # å»é™¤ä»»ä½•å¯èƒ½çš„é‡å¤ç´¢å¼•
                            if clean_df.index.has_duplicates:
                                clean_df = clean_df.loc[~clean_df.index.duplicated(keep='first')]
                                clean_df = clean_df.reset_index(drop=True)
                            
                            # è½¬æ¢ä¸ºå­—å…¸å†è½¬å›DataFrameï¼ˆæœ€å½»åº•çš„æ¸…ç†ï¼‰
                            # ä¿æŠ¤é‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—
                            protected_cols = ['Symbol', 'Ticker', 'Name', 'ENName', 'Code', 'Date']
                            data_dict = {}
                            for col in clean_df.columns:
                                try:
                                    if col in protected_cols:
                                        # å¯¹äºé‡è¦çš„å­—ç¬¦ä¸²åˆ—å’Œæ—¥æœŸåˆ—ï¼Œç›´æ¥ä¿ç•™åŸå€¼
                                        data_dict[col] = clean_df[col].tolist()
                                    else:
                                        data_dict[col] = clean_df[col].values.tolist()
                                except:
                                    if col in protected_cols:
                                        # å¯¹äºé‡è¦åˆ—ï¼Œå°è¯•ä¿ç•™åŸå€¼è€Œä¸æ˜¯è®¾ä¸ºNaN
                                        try:
                                            data_dict[col] = clean_df[col].astype(str).tolist()
                                        except:
                                            data_dict[col] = [''] * len(clean_df)
                                    else:
                                        data_dict[col] = [np.nan] * len(clean_df)
                            
                            clean_df = pd.DataFrame(data_dict)
                            
                            if combined_df is None:
                                combined_df = clean_df
                            else:
                                # é€ä¸ªæ·»åŠ è¡Œ
                                combined_df = pd.concat([combined_df, clean_df], ignore_index=True, sort=False)
                            
                            successful_merges += 1
                            logger.debug(f"æˆåŠŸåˆå¹¶ç¬¬ {i+1} ä¸ªDataFrame")
                            
                        except Exception as merge_error:
                            logger.warning(f"è·³è¿‡ç¬¬ {i+1} ä¸ªDataFrameï¼Œåˆå¹¶å¤±è´¥: {merge_error}")
                            continue
                    
                    if combined_df is not None and not combined_df.empty:
                        logger.info(f"âœ… é™çº§åˆå¹¶æˆåŠŸ: {successful_merges}/{len(valid_results)} ä¸ªç»“æœ")
                        # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                        indicator_count = len(combined_df.columns) - (2 if 'Date' in combined_df.columns else 1)
                        logger.info(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°é‡: {indicator_count}")
                        logger.info(f"ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
                        return combined_df
                    else:
                        logger.error("âŒ é™çº§åˆå¹¶åç»“æœä¸ºç©º")
                        return pd.DataFrame()
                    
                except Exception as e2:
                    logger.error(f"âŒ é™çº§åˆå¹¶ä¹Ÿå¤±è´¥: {e2}")
                    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šä¿å­˜å•ä¸ªæœ€å¤§çš„ç»“æœ
                    try:
                        if all_results:
                            largest_df = max(all_results, key=lambda x: len(x) if x is not None else 0)
                            if largest_df is not None and not largest_df.empty:
                                result_df = largest_df.copy().reset_index(drop=True)
                                logger.warning(f"âš ï¸ ä½¿ç”¨æœ€å¤§çš„å•ä¸ªç»“æœ: {len(result_df)} è¡Œ")
                                return result_df
                    except:
                        pass
                    return pd.DataFrame()
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸè®¡ç®—ä»»ä½•è‚¡ç¥¨çš„æŒ‡æ ‡")
            return pd.DataFrame()
    
    def _calculate_all_stocks_sequential(self, stocks: List[str]) -> pd.DataFrame:
        """é¡ºåºè®¡ç®—å¤šåªè‚¡ç¥¨çš„æŒ‡æ ‡"""
        logger.info(f"ä½¿ç”¨é¡ºåºæ¨¡å¼è®¡ç®— {len(stocks)} åªè‚¡ç¥¨")
        
        all_results = []
        success_count = 0
        start_time = time.time()
        
        for i, symbol in enumerate(stocks, 1):
            stock_start_time = time.time()
            logger.info(f"ğŸ“ˆ å¤„ç†ç¬¬ {i}/{len(stocks)} åªè‚¡ç¥¨: {symbol}")
            
            result = self.calculate_all_indicators_for_stock(symbol)
            if result is not None:
                all_results.append(result)
                success_count += 1
                stock_elapsed = time.time() - stock_start_time
                # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                indicator_count = len(result.columns) - (2 if 'Date' in result.columns else 1)
                logger.info(f"âœ… {symbol}: å®Œæˆ {indicator_count} ä¸ªæŒ‡æ ‡ (è€—æ—¶: {stock_elapsed:.2f}s)")
            else:
                logger.warning(f"âš ï¸ {symbol}: è®¡ç®—å¤±è´¥")
        
        elapsed_time = time.time() - start_time
        
        if all_results:
            try:
                # æ•°æ®åˆå¹¶å‰çš„é¢„å¤„ç†å’Œæ£€æŸ¥
                logger.info("å¼€å§‹åˆå¹¶å¤šåªè‚¡ç¥¨çš„è®¡ç®—ç»“æœ...")
                
                # é‡ç½®æ‰€æœ‰DataFrameçš„ç´¢å¼•ä»¥é¿å…å†²çª
                cleaned_results = []
                for i, df in enumerate(all_results):
                    if df is None or df.empty:
                        logger.warning(f"è·³è¿‡ç©ºçš„DataFrame (ç´¢å¼•: {i})")
                        continue
                    cleaned_results.append(df.reset_index(drop=True))
                
                if not cleaned_results:
                    logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®¡ç®—ç»“æœå¯ä»¥åˆå¹¶")
                    return pd.DataFrame()
                
                # å®‰å…¨åˆå¹¶DataFrame
                combined_df = pd.concat(cleaned_results, ignore_index=True, sort=False)
                
                # æ£€æŸ¥é‡å¤è¡Œ
                initial_rows = len(combined_df)
                combined_df = combined_df.drop_duplicates()
                if len(combined_df) < initial_rows:
                    logger.info(f"ç§»é™¤äº† {initial_rows - len(combined_df)} è¡Œé‡å¤æ•°æ®")
                
                logger.info(f"âœ… é¡ºåºè®¡ç®—å®Œæˆ: {success_count}/{len(stocks)} åªè‚¡ç¥¨æˆåŠŸ (æ€»è€—æ—¶: {elapsed_time:.2f}s)")
                # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
                indicator_count = len(combined_df.columns) - (2 if 'Date' in combined_df.columns else 1)
                logger.info(f"ğŸ“Š æ€»æŒ‡æ ‡æ•°é‡: {indicator_count}")
                logger.info(f"ğŸ“ˆ æ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
                logger.info(f"â±ï¸ å¹³å‡æ¯åªè‚¡ç¥¨è€—æ—¶: {elapsed_time/len(stocks):.2f}s")
                return combined_df
                
            except Exception as e:
                logger.error(f"âŒ é¡ºåºè®¡ç®—åˆå¹¶ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                return pd.DataFrame()
        else:
            logger.error("âŒ æ²¡æœ‰æˆåŠŸè®¡ç®—ä»»ä½•è‚¡ç¥¨çš„æŒ‡æ ‡")
            return pd.DataFrame()
    
    def save_results(self, df: pd.DataFrame, filename: str = "enhanced_quantitative_indicators.csv") -> str:
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶ï¼ŒåŒ…å«ä¸­æ–‡æ ‡ç­¾è¡Œï¼Œç©ºå€¼ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ï¼ˆå…¼å®¹SASï¼‰"""
        if df.empty:
            logger.warning("DataFrameä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return ""
        
        try:
            output_path = self.output_dir / filename
            
            # è·å–åˆ—åå’Œä¸­æ–‡æ ‡ç­¾
            columns = df.columns.tolist()
            chinese_labels = self.get_field_labels(columns)
            
            # å¤„ç†ç©ºå€¼ï¼šå°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä»¥å…¼å®¹SAS
            df_clean = df.copy()
            df_clean = df_clean.fillna('')  # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            
            logger.info("ğŸ“ ç©ºå€¼å¤„ç†: å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ä»¥å…¼å®¹SAS")
            
            # ä½¿ç”¨æ‰‹åŠ¨æ–¹å¼å†™å…¥CSVæ–‡ä»¶ä»¥åŒ…å«ä¸­æ–‡æ ‡ç­¾è¡Œ
            with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
                import csv
                writer = csv.writer(f)
                
                # ç¬¬ä¸€è¡Œï¼šå­—æ®µåï¼ˆè‹±æ–‡åˆ—åï¼‰
                writer.writerow(columns)
                
                # ç¬¬äºŒè¡Œï¼šä¸­æ–‡æ ‡ç­¾
                writer.writerow(chinese_labels)
                
                # ç¬¬ä¸‰è¡Œå¼€å§‹ï¼šå…·ä½“æ•°æ®
                for _, row in df_clean.iterrows():
                    writer.writerow(row.values)
            
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            logger.info(f"åŒ…å«ä¸­æ–‡æ ‡ç­¾è¡Œçš„CSVæ ¼å¼:")
            logger.info(f"  ç¬¬ä¸€è¡Œ: å­—æ®µå ({len(columns)} ä¸ªå­—æ®µ)")
            logger.info(f"  ç¬¬äºŒè¡Œ: ä¸­æ–‡æ ‡ç­¾")
            logger.info(f"  ç¬¬ä¸‰è¡Œå¼€å§‹: å…·ä½“æ•°æ® ({len(df)} è¡Œæ•°æ®)")
            logger.info(f"  ç©ºå€¼å¤„ç†: NaN â†’ '' (ç©ºå­—ç¬¦ä¸²ï¼Œå…¼å®¹SAS)")
            
            # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
            indicator_count = len(df.columns) - (2 if 'Date' in df.columns else 1)
            logger.info(f"æŒ‡æ ‡æ•°é‡: {indicator_count}")
            
            return str(output_path)
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return ""
    
    def run(self, max_stocks: Optional[int] = None, output_filename: str = "enhanced_quantitative_indicators.csv"):
        """è¿è¡Œå®Œæ•´çš„æŒ‡æ ‡è®¡ç®—æµç¨‹"""
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå¢å¼ºç‰ˆQlibæŒ‡æ ‡è®¡ç®—å™¨")
        logger.info(f"âš™ï¸ å¤šçº¿ç¨‹æ¨¡å¼: {'å¯ç”¨' if self.enable_parallel else 'ç¦ç”¨'}")
        if self.enable_parallel:
            logger.info(f"ğŸ§µ æœ€å¤§çº¿ç¨‹æ•°: {self.max_workers}")
        logger.info("=" * 80)
        
        start_time = time.time()
        
        # è®¡ç®—æŒ‡æ ‡
        results_df = self.calculate_all_indicators(max_stocks=max_stocks)
        
        total_elapsed = time.time() - start_time
        
        if not results_df.empty:
            # ä¿å­˜ç»“æœ
            output_path = self.save_results(results_df, output_filename)
            
            logger.info("=" * 80)
            logger.info("âœ… æŒ‡æ ‡è®¡ç®—å®Œæˆï¼")
            # è®¡ç®—æŒ‡æ ‡æ•°é‡ï¼šæ€»åˆ—æ•°å‡å»Dateå’ŒSymbolåˆ—
            indicator_count = len(results_df.columns) - (2 if 'Date' in results_df.columns else 1)
            logger.info(f"ğŸ“Š æ€»å…±è®¡ç®—äº† {indicator_count} ä¸ªæŒ‡æ ‡")
            logger.info(f"ğŸ“ˆ åŒ…å« {results_df['Symbol'].nunique()} åªè‚¡ç¥¨")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_elapsed:.2f} ç§’")
            logger.info(f"ğŸ’¾ ç»“æœä¿å­˜è‡³: {output_path}")
            logger.info("=" * 80)
            
            # æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡
            self._show_indicators_summary(results_df)
        else:
            logger.error(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ (è€—æ—¶: {total_elapsed:.2f}s)")
    
    def _show_indicators_summary(self, df: pd.DataFrame):
        """æ˜¾ç¤ºæŒ‡æ ‡ç»Ÿè®¡æ‘˜è¦"""
        logger.info("ğŸ“Š æŒ‡æ ‡åˆ†ç±»ç»Ÿè®¡:")
        logger.info("-" * 50)
        
        # ç»Ÿè®¡å„ç±»æŒ‡æ ‡æ•°é‡
        alpha158_count = len([col for col in df.columns if col.startswith('ALPHA158_')])
        alpha360_count = len([col for col in df.columns if col.startswith('ALPHA360_')])
        technical_count = len([col for col in df.columns if any(prefix in col for prefix in ['SMA', 'EMA', 'RSI', 'MACD', 'ADX', 'ATR', 'BB_', 'STOCH']) and not col.startswith(('ALPHA158_', 'ALPHA360_'))])
        candlestick_count = len([col for col in df.columns if col.startswith('CDL')])
        financial_count = len([col for col in df.columns if any(prefix in col for prefix in ['PriceToBook', 'MarketCap', 'PE', 'ROE', 'ROA', 'turnover', 'TobinsQ'])])
        volatility_count = len([col for col in df.columns if 'Volatility' in col or 'SemiDeviation' in col])
        
        logger.info(f"Alpha158æŒ‡æ ‡: {alpha158_count} ä¸ª")
        logger.info(f"Alpha360æŒ‡æ ‡: {alpha360_count} ä¸ª")
        logger.info(f"æŠ€æœ¯æŒ‡æ ‡: {technical_count} ä¸ª")
        logger.info(f"èœ¡çƒ›å›¾å½¢æ€: {candlestick_count} ä¸ª")
        logger.info(f"è´¢åŠ¡æŒ‡æ ‡: {financial_count} ä¸ª")
        logger.info(f"æ³¢åŠ¨ç‡æŒ‡æ ‡: {volatility_count} ä¸ª")
        logger.info(f"æ€»è®¡: {alpha158_count + alpha360_count + technical_count + candlestick_count + financial_count + volatility_count} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¢å¼ºç‰ˆQlibæŒ‡æ ‡è®¡ç®—å™¨ - é›†æˆAlpha158ã€Alpha360æŒ‡æ ‡ä½“ç³»',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹:
  # è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„æ‰€æœ‰æŒ‡æ ‡
  python qlib_indicators.py
  
  # åªè®¡ç®—å‰10åªè‚¡ç¥¨
  python qlib_indicators.py --max-stocks 10
  
  # æŒ‡å®šæ•°æ®ç›®å½•å’Œè´¢åŠ¡æ•°æ®ç›®å½•
  python qlib_indicators.py --data-dir ./data --financial-dir ./financial_data
  
  # è‡ªå®šä¹‰è¾“å‡ºæ–‡ä»¶å
  python qlib_indicators.py --output indicators_2025.csv
  
  # ç¦ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—
  python qlib_indicators.py --disable-parallel
  
  # è‡ªå®šä¹‰çº¿ç¨‹æ•°é‡
  python qlib_indicators.py --max-workers 16
  
  # è°ƒè¯•æ¨¡å¼
  python qlib_indicators.py --log-level DEBUG --max-stocks 5

å¤šçº¿ç¨‹æ€§èƒ½ä¼˜åŒ–:
  - ğŸš€ å¤šåªè‚¡ç¥¨å¹¶è¡Œè®¡ç®—: æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦
  - âš¡ å•åªè‚¡ç¥¨æŒ‡æ ‡ç±»å‹å¹¶è¡Œ: Alpha158ã€Alpha360ã€æŠ€æœ¯æŒ‡æ ‡ç­‰åŒæ—¶è®¡ç®—
  - ğŸ§µ æ™ºèƒ½çº¿ç¨‹ç®¡ç†: è‡ªåŠ¨ä¼˜åŒ–çº¿ç¨‹æ•°é‡ï¼Œé¿å…èµ„æºç«äº‰
  - ğŸ“Š å®æ—¶è¿›åº¦æ˜¾ç¤º: è¯¦ç»†çš„è®¡ç®—è¿›åº¦å’Œæ€§èƒ½ç»Ÿè®¡
  - ğŸ”’ çº¿ç¨‹å®‰å…¨: ç¡®ä¿æŒ‡æ ‡å»é‡å’Œæ•°æ®ä¸€è‡´æ€§

æ”¯æŒçš„æŒ‡æ ‡ç±»å‹:
  - Alpha158æŒ‡æ ‡ä½“ç³»: ~158ä¸ª (KBARã€ä»·æ ¼ã€æˆäº¤é‡ã€æ»šåŠ¨æŠ€æœ¯æŒ‡æ ‡)
  - Alpha360æŒ‡æ ‡ä½“ç³»: ~360ä¸ª (è¿‡å»60å¤©æ ‡å‡†åŒ–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®)
  - æŠ€æœ¯æŒ‡æ ‡: ~60ä¸ª (ç§»åŠ¨å¹³å‡ã€MACDã€RSIã€å¸ƒæ—å¸¦ç­‰)
  - èœ¡çƒ›å›¾å½¢æ€: 61ä¸ª (é”¤å­çº¿ã€åå­—æ˜Ÿã€åæ²¡å½¢æ€ç­‰)
  - è´¢åŠ¡æŒ‡æ ‡: ~15ä¸ª (å¸‚å‡€ç‡ã€æ¢æ‰‹ç‡ã€æ‰˜å®¾Qå€¼ç­‰)
  - æ³¢åŠ¨ç‡æŒ‡æ ‡: ~8ä¸ª (å·²å®ç°æ³¢åŠ¨ç‡ã€åŠå˜å·®ç­‰)
  
æ€»è®¡çº¦650+ä¸ªæŒ‡æ ‡ï¼Œå…·å¤‡å»é‡åŠŸèƒ½å’Œå¤šçº¿ç¨‹åŠ é€Ÿ
        '''
    )
    
    parser.add_argument(
        '--data-dir',
        default=r"D:\stk_data\trd\us_data",
        help='Qlibæ•°æ®ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--financial-dir',
        help='è´¢åŠ¡æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ~/.qlib/financial_data)'
    )
    
    parser.add_argument(
        '--max-stocks',
        type=int,
        help='æœ€å¤§è‚¡ç¥¨æ•°é‡é™åˆ¶'
    )
    
    parser.add_argument(
        '--output',
        default='enhanced_quantitative_indicators.csv',
        help='è¾“å‡ºæ–‡ä»¶å'
    )
    
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ«'
    )
    
    parser.add_argument(
        '--disable-parallel',
        action='store_true',
        help='ç¦ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè®¡ç®—'
    )
    
    parser.add_argument(
        '--max-workers',
        type=int,
        help='æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°+4ï¼Œæœ€å¤§32)'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.remove()
    logger.add(
        lambda msg: print(msg, end=""),
        level=args.log_level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
    )
    
    try:
        # åˆ›å»ºè®¡ç®—å™¨å¹¶è¿è¡Œ
        calculator = QlibIndicatorsEnhancedCalculator(
            data_dir=args.data_dir,
            financial_data_dir=args.financial_dir,
            enable_parallel=not args.disable_parallel,
            max_workers=args.max_workers
        )
        
        calculator.run(
            max_stocks=args.max_stocks,
            output_filename=args.output
        )
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
